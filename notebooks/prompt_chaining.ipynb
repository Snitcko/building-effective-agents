{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cdc1dd-28ad-4e2f-84bc-3e7cc09dc8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from typing import List, Dict, Optional\n",
    "from pydantic import BaseModel, HttpUrl\n",
    "import json\n",
    "import requests\n",
    "\n",
    "#import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ea8eb2-57eb-4486-998f-9be6c3f42293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "index_name = \"augmentedllm\"\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "_set_env(\"PINECONE_API_KEY\")\n",
    "_set_env(\"JINA_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31141e31-10f7-4972-9ec7-bf7c53087d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Models\n",
    "class ArticleContent(BaseModel):\n",
    "    \"\"\"Represents scraped and processed article content\"\"\"\n",
    "    url: HttpUrl\n",
    "    title: str\n",
    "    content: str\n",
    "    metadata: Dict\n",
    "\n",
    "class ContentBrief(BaseModel):\n",
    "    \"\"\"Represents the content brief/specification\"\"\"\n",
    "    main_topics: List[str]\n",
    "    target_audience: str\n",
    "    key_messages: List[str]\n",
    "    tone_guidelines: str\n",
    "    hashtags: List[str]\n",
    "    constraints: Dict[str, str]  # platform-specific constraints\n",
    "    source_article: ArticleContent\n",
    "\n",
    "class PostDraft(BaseModel):\n",
    "    \"\"\"Represents the initial post draft\"\"\"\n",
    "    content: str\n",
    "    hashtags: List[str]\n",
    "    brief: ContentBrief\n",
    "    metadata: Dict\n",
    "\n",
    "class FinalPost(BaseModel):\n",
    "    \"\"\"Represents the final optimized post\"\"\"\n",
    "    content: Dict[str, str]  # language -> content mapping\n",
    "    hashtags: List[str]\n",
    "    engagement_metrics: Dict[str, float]  # predicted engagement metrics\n",
    "    platform_specific: Dict[str, Dict]  # platform-specific formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eb76f4-a0d9-4505-af46-7424a24ee02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScraper:\n",
    "    \"\"\"Handles article scraping using Jina Reader API\"\"\"\n",
    "    \n",
    "    def __init__(self, jina_api_key: str):\n",
    "        self.api_key = jina_api_key\n",
    "        self.api_url = \"https://r.jina.ai\"\n",
    "\n",
    "    async def process_response(self, response) -> dict:\n",
    "        \"\"\"Process API response and return structured data\"\"\"\n",
    "        try:\n",
    "            if response.status_code == 200:\n",
    "                return {\n",
    "                    'text': response.text,\n",
    "                    'status': 'success'\n",
    "                }\n",
    "            else:\n",
    "                print(f\"Error status: {response.status_code}\")\n",
    "                print(f\"Response headers: {response.headers}\")\n",
    "                print(f\"Response content: {response.text[:500]}\")\n",
    "                response.raise_for_status()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing response: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "    async def scrape(self, url: str) -> ArticleContent:\n",
    "        \"\"\"Scrape article content from URL using Jina Reader API\"\"\"\n",
    "        try:\n",
    "            headers = {\n",
    "                \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"X-Return-Format\": \"text\"\n",
    "            }\n",
    "            \n",
    "            data = {\n",
    "                \"url\": url\n",
    "            }\n",
    "            \n",
    "            print(f\"Making request to Jina AI...\")\n",
    "            print(f\"URL to scrape: {url}\")\n",
    "            response = requests.post(\n",
    "                self.api_url,\n",
    "                headers=headers,\n",
    "                json=data \n",
    "            )\n",
    "            \n",
    "            result = await self.process_response(response)\n",
    "            \n",
    "            return ArticleContent(\n",
    "                url=url,\n",
    "                title=url.split('/')[-1] or url,\n",
    "                content=result['text'],\n",
    "                metadata={\n",
    "                    \"source\": \"jina_reader\",\n",
    "                    \"scrape_time\": \"now\",\n",
    "                    \"status\": result['status']\n",
    "                }\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Full error details: {str(e)}\")\n",
    "            raise ValueError(f\"Failed to scrape URL {url}: {str(e)}\")\n",
    "\n",
    "class DocumentProcessor:\n",
    "    \"\"\"Processes documents into chunks.\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "\n",
    "    def create_chunks(self, text: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Split text into overlapping chunks.\"\"\"\n",
    "        if not text:\n",
    "            raise ValueError(\"Empty text provided\")\n",
    "        if self.chunk_size <= self.chunk_overlap:\n",
    "            raise ValueError(\"chunk_size must be greater than chunk_overlap\")\n",
    "        \n",
    "        chunks = []\n",
    "        start = 0\n",
    "        \n",
    "        while start < len(text):\n",
    "            end = start + self.chunk_size\n",
    "            chunk = text[start:end]\n",
    "            \n",
    "            chunks.append({\n",
    "                'text': chunk,\n",
    "                'metadata': {'start_char': start, 'end_char': end}\n",
    "            })\n",
    "            \n",
    "            start += self.chunk_size - self.chunk_overlap\n",
    "            \n",
    "        print(f\"\\nCreated {len(chunks)} chunks\")\n",
    "        print(f\"Sample chunk (first 50 chars): {chunks[0]['text'][:50]}...\")\n",
    "        return chunks\n",
    "\n",
    "class EmbeddingGenerator:\n",
    "    \"\"\"Handles text embedding generation.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.api_url = \"https://api.openai.com/v1/embeddings\"\n",
    "        \n",
    "    def create_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"Generate embedding for text.\"\"\"\n",
    "        if not text.strip():\n",
    "            raise ValueError(\"Empty text provided\")\n",
    "            \n",
    "        response = requests.post(\n",
    "            self.api_url,\n",
    "            headers={\n",
    "                \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            },\n",
    "            json={\n",
    "                \"model\": \"text-embedding-3-small\",\n",
    "                \"input\": text\n",
    "            }\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        embedding = response.json()['data'][0]['embedding']\n",
    "        print(f\"Generated embedding, sample of first 3 dimensions: {embedding[:3]}\")\n",
    "        return embedding\n",
    "\n",
    "class VectorStore:\n",
    "    \"\"\"Manages vector storage operations.\"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str, index_name: str):\n",
    "        self.api_key = api_key\n",
    "        self.index_name = index_name\n",
    "        self.headers = {\n",
    "            \"Api-Key\": self.api_key,\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"X-Pinecone-API-Version\": \"2024-07\"\n",
    "        }\n",
    "        self.index_host_cache = {}\n",
    "        \n",
    "    def describe_index(self) -> str:\n",
    "        \"\"\"Get or retrieve cached index host.\"\"\"\n",
    "        if self.index_name in self.index_host_cache:\n",
    "            return self.index_host_cache[self.index_name]\n",
    "            \n",
    "        url = f\"https://api.pinecone.io/indexes/{self.index_name}\"\n",
    "        response = requests.get(url, headers=self.headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        host = response.json()[\"host\"]\n",
    "        self.index_host_cache[self.index_name] = host\n",
    "        print(f\"\\nConnected to Pinecone host: {host}\")\n",
    "        return host\n",
    "        \n",
    "    def query_vectors(self, \n",
    "                      query_vector: List[float], \n",
    "                      top_k: int = 3, \n",
    "                      namespace: str = \"\") -> List[Dict]:\n",
    "        \"\"\"Query for most similar vectors.\"\"\"\n",
    "        host = self.describe_index()\n",
    "        url = f\"https://{host}/query\"\n",
    "        data = {\n",
    "            \"vector\": query_vector,\n",
    "            \"topK\": top_k,\n",
    "            \"namespace\": namespace,\n",
    "            \"includeMetadata\": True\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nQuerying Pinecone for top {top_k} matches\")\n",
    "        response = requests.post(url, headers=self.headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        matches = response.json()['matches']\n",
    "        return matches\n",
    "        \n",
    "    def store_vectors(self, vectors: List[Dict], namespace: str = \"\") -> Dict:\n",
    "        \"\"\"Store vectors in Pinecone.\"\"\"\n",
    "        host = self.describe_index()\n",
    "        url = f\"https://{host}/vectors/upsert\"\n",
    "        data = {\n",
    "            \"vectors\": vectors,\n",
    "            \"namespace\": namespace\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nUploading {len(vectors)} vectors to Pinecone\")\n",
    "        print(f\"Sample vector metadata: {vectors[0]['metadata']}\")\n",
    "        \n",
    "        response = requests.post(url, headers=self.headers, json=data)    \n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        print(f\"Pinecone response: {result}\")\n",
    "        return result\n",
    "\n",
    "class RagProcessor:\n",
    "    \"\"\"Handles RAG operations\"\"\"\n",
    "    \n",
    "    def __init__(self, openai_key: str, pinecone_key: str, index_name: str):\n",
    "        self.processor = DocumentProcessor()\n",
    "        self.embedding_gen = EmbeddingGenerator(openai_key)\n",
    "        self.vector_store = VectorStore(pinecone_key, index_name)\n",
    "    \n",
    "    async def store_article(self, article: ArticleContent) -> str:\n",
    "        \"\"\"Process article and store in vector database\"\"\"\n",
    "        try:\n",
    "            # Create chunks\n",
    "            chunks = self.processor.create_chunks(article.content)\n",
    "            \n",
    "            # Generate embeddings and prepare vectors\n",
    "            vectors = []\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                embedding = self.embedding_gen.create_embedding(chunk['text'])\n",
    "                if embedding:\n",
    "                    vector_id = f\"article_{hash(article.url)}_{i}\"\n",
    "                    vectors.append({\n",
    "                        'id': vector_id,\n",
    "                        'values': embedding,\n",
    "                        'metadata': {\n",
    "                            'text': chunk['text'],\n",
    "                            'url': str(article.url),\n",
    "                            'title': article.title,\n",
    "                            **chunk['metadata']\n",
    "                        }\n",
    "                    })\n",
    "            \n",
    "            # Store vectors\n",
    "            if vectors:\n",
    "                self.vector_store.store_vectors(vectors)\n",
    "                return vectors[0]['id']  # Return first chunk ID\n",
    "            return \"\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to store article: {str(e)}\")\n",
    "    \n",
    "    async def get_context(self, query: str, top_k: int = 3) -> List[Dict]:\n",
    "        \"\"\"Get relevant context for query\"\"\"\n",
    "        try:\n",
    "            query_embedding = self.embedding_gen.create_embedding(query)\n",
    "            return self.vector_store.query_vectors(query_embedding, top_k)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to get context: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06ba915-606b-49ec-8809-646c91e7aa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentAnalyst:\n",
    "    \"\"\"Creates content brief from article\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, rag_processor=None, model: str = \"gpt-3.5-turbo\"):\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "        self.rag_processor = rag_processor\n",
    "        \n",
    "    async def process(self, article: ArticleContent) -> ContentBrief:\n",
    "        try:\n",
    "            # Get relevant context if RAG is available\n",
    "            context = \"\"\n",
    "            if self.rag_processor:\n",
    "                matches = await self.rag_processor.get_context(article.content)\n",
    "                context = \"\\n\\n\".join(m[\"metadata\"].get(\"text\", \"\") for m in matches if \"metadata\" in m)\n",
    "            \n",
    "            # Make API request\n",
    "            response = requests.post(\n",
    "                \"https://api.openai.com/v1/chat/completions\",\n",
    "                headers={\n",
    "                    \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "                    \"Content-Type\": \"application/json\"\n",
    "                },\n",
    "                json={\n",
    "                    \"model\": self.model,\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": \"\"\"Create a social media brief in this exact JSON format:\n",
    "                            {\n",
    "                                \"main_topics\": [\"topic1\", \"topic2\"],\n",
    "                                \"target_audience\": \"description\",\n",
    "                                \"key_messages\": [\"msg1\", \"msg2\"],\n",
    "                                \"tone_guidelines\": \"description\",\n",
    "                                \"hashtags\": [\"#tag1\", \"#tag2\"],\n",
    "                                \"constraints\": {\"platform\": \"rules\"}\n",
    "                            }\"\"\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": f\"Article: {article.content[:2000]}\"\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            data = response.json()\n",
    "            content = data['choices'][0]['message']['content']\n",
    "            print(f\"Raw content from API: {content}\") \n",
    "            \n",
    "            result = json.loads(content)\n",
    "            \n",
    "            return ContentBrief(\n",
    "                main_topics=result['main_topics'],\n",
    "                target_audience=result['target_audience'],\n",
    "                key_messages=result['key_messages'],\n",
    "                tone_guidelines=result['tone_guidelines'],\n",
    "                hashtags=result['hashtags'],\n",
    "                constraints=result['constraints'],\n",
    "                source_article=article\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "class ContentOptimizer:\n",
    "    \"\"\"Optimizes and translates post\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, model: str = \"gpt-3.5-turbo\"):\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "        \n",
    "    async def process(self, draft: PostDraft) -> FinalPost:\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                \"https://api.openai.com/v1/chat/completions\",\n",
    "                headers={\n",
    "                    \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "                    \"Content-Type\": \"application/json\"\n",
    "                },\n",
    "                json={\n",
    "                    \"model\": self.model,\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": \"\"\"Optimize and translate this post. Return JSON in format:\n",
    "                            {\n",
    "                                \"translations\": {\n",
    "                                    \"en\": \"english text\",\n",
    "                                    \"es\": \"spanish text\"\n",
    "                                },\n",
    "                                \"engagement_metrics\": {\n",
    "                                    \"expected_likes\": 100,\n",
    "                                    \"expected_shares\": 50\n",
    "                                },\n",
    "                                \"platform_versions\": {\n",
    "                                    \"twitter\": {\"text\": \"twitter text\", \"type\": \"tweet\"},\n",
    "                                    \"linkedin\": {\"text\": \"linkedin text\", \"type\": \"post\"}\n",
    "                                }\n",
    "                            }\"\"\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": f\"\"\"Post: {draft.content}\n",
    "                            Tone: {draft.brief.tone_guidelines}\n",
    "                            Audience: {draft.brief.target_audience}\"\"\"\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            data = response.json()\n",
    "            content = data['choices'][0]['message']['content']\n",
    "            print(f\"Raw content from API: {content}\")\n",
    "            result = json.loads(content)\n",
    "            \n",
    "            return FinalPost(\n",
    "                content=result['translations'],\n",
    "                hashtags=draft.hashtags,\n",
    "                engagement_metrics=result['engagement_metrics'],\n",
    "                platform_specific=result['platform_versions']\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06270f50-6d2b-4ceb-8e9e-63a01a26938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentPipeline:\n",
    "    \"\"\"Orchestrates the content processing pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, openai_key: str, jina_key: str, pinecone_key: str, index_name: str):\n",
    "        # Initialize components\n",
    "        self.scraper = WebScraper(jina_key)\n",
    "        self.rag = RagProcessor(openai_key, pinecone_key, index_name)\n",
    "        \n",
    "        # Initialize agents with RAG\n",
    "        self.analyst = ContentAnalyst(openai_key, self.rag)\n",
    "        self.writer = ContentWriter(openai_key, self.rag)\n",
    "        self.optimizer = ContentOptimizer(openai_key)\n",
    "    \n",
    "    async def process_url(self, url: str) -> FinalPost:\n",
    "        \"\"\"Process URL through the pipeline\"\"\"\n",
    "        try:\n",
    "            # 1. Scrape article\n",
    "            print(\"1. Scraping article...\")\n",
    "            article = await self.scraper.scrape(url)\n",
    "            print(\"‚úì Article scraped\")\n",
    "            \n",
    "            # 2. Store in RAG system\n",
    "            print(\"2. Storing in RAG system...\")\n",
    "            await self.rag.store_article(article)\n",
    "            print(\"‚úì Article stored in RAG\")\n",
    "            \n",
    "            # 3. Generate brief\n",
    "            print(\"3. Generating content brief...\")\n",
    "            brief = await self.analyst.process(article)\n",
    "            print(\"‚úì Content brief created\")\n",
    "            \n",
    "            # 4. Create draft\n",
    "            print(\"4. Creating post draft...\")\n",
    "            draft = await self.writer.process(brief)\n",
    "            print(\"‚úì Post draft created\")\n",
    "            \n",
    "            # 5. Optimize and finalize\n",
    "            print(\"5. Optimizing content...\")\n",
    "            final = await self.optimizer.process(draft)\n",
    "            print(\"‚úì Content optimized\")\n",
    "            \n",
    "            return final\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Pipeline failed: {str(e)}\"\n",
    "            print(f\"‚ùå {error_msg}\")\n",
    "            raise ValueError(error_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb45ad7-6614-4e31-a0a1-bb829dc79f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_pipeline(openai_key: str, jina_key: str, pinecone_key: str, index_name: str):\n",
    "    \"\"\"Test the entire pipeline\"\"\"\n",
    "    \n",
    "    # Initialize pipeline\n",
    "    pipeline = ContentPipeline(openai_key, jina_key, pinecone_key, index_name)\n",
    "    \n",
    "    # Test URL\n",
    "    test_url = \"https://example.com/article\"\n",
    "    \n",
    "    try:\n",
    "        # Process URL\n",
    "        result = await pipeline.process_url(test_url)\n",
    "        print(\"\\nPipeline Test Results:\")\n",
    "        print(f\"- Content languages: {list(result.content.keys())}\")\n",
    "        print(f\"- Number of hashtags: {len(result.hashtags)}\")\n",
    "        print(f\"- Platform versions: {list(result.platform_specific.keys())}\")\n",
    "        print(\"\\n‚úÖ Pipeline test completed successfully\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Pipeline test failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "async def test_individual_components(openai_key: str, jina_key: str, pinecone_key: str, index_name: str):\n",
    "    \"\"\"Test each component individually\"\"\"\n",
    "    \n",
    "    print(\"\\nTesting Individual Components:\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Test WebScraper\n",
    "        print(\"\\nTesting WebScraper...\")\n",
    "        scraper = WebScraper(jina_key)\n",
    "        article = await scraper.scrape(\"https://example.com/article\")\n",
    "        print(\"‚úì WebScraper test passed\")\n",
    "        \n",
    "        # 2. Test RagProcessor\n",
    "        print(\"\\nTesting RagProcessor...\")\n",
    "        rag = RagProcessor(openai_key, pinecone_key, index_name)\n",
    "        vector_id = await rag.store_article(article)\n",
    "        context = await rag.get_context(\"test query\")\n",
    "        print(\"‚úì RagProcessor test passed\")\n",
    "        \n",
    "        # 3. Test ContentAnalyst\n",
    "        print(\"\\nTesting ContentAnalyst...\")\n",
    "        analyst = ContentAnalyst(openai_key, rag)\n",
    "        brief = await analyst.process(article)\n",
    "        print(\"‚úì ContentAnalyst test passed\")\n",
    "        \n",
    "        # 4. Test ContentWriter\n",
    "        print(\"\\nTesting ContentWriter...\")\n",
    "        writer = ContentWriter(openai_key, rag)\n",
    "        draft = await writer.process(brief)\n",
    "        print(\"‚úì ContentWriter test passed\")\n",
    "        \n",
    "        # 5. Test ContentOptimizer\n",
    "        print(\"\\nTesting ContentOptimizer...\")\n",
    "        optimizer = ContentOptimizer(openai_key)\n",
    "        final = await optimizer.optimizer.process(draft)\n",
    "        print(\"‚úì ContentOptimizer test passed\")\n",
    "        \n",
    "        print(\"\\n‚úÖ All component tests completed successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Component testing failed: {str(e)}\")\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a5f08d-cd9b-4659-87ff-16248e0f6494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage in Jupyter notebook:\n",
    "async def run_tests(openai_key: str, jina_key: str, pinecone_key: str, index_name: str):\n",
    "    print(\"üöÄ Starting tests...\")\n",
    "    await test_pipeline(openai_key, jina_key, pinecone_key, index_name)\n",
    "    await test_individual_components(openai_key, jina_key, pinecone_key, index_name)\n",
    "    print(\"\\n‚ú® All tests completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b8e30f-824f-4cea-8455-abb00cb36495",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_url = \"https://www.anthropic.com/research/building-effective-agents\"  \n",
    "\n",
    "pipeline = ContentPipeline(\n",
    "    openai_key=openai_key,\n",
    "    jina_key=jina_api_key,\n",
    "    pinecone_key=pinecone_key,\n",
    "    index_name=index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819a2379-ece0-4aca-bfbd-d5a3a8baca02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test WebScraper\n",
    "async def test_single_url():\n",
    "    scraper = WebScraper(jina_api_key)\n",
    "    url = \"https://www.anthropic.com/research/building-effective-agents\"\n",
    "    \n",
    "    print(\"üîç Testing WebScraper...\")\n",
    "    print(f\"URL: {url}\")\n",
    "    try:\n",
    "        result = await scraper.scrape(url)\n",
    "        print(\"\\n‚úÖ Success!\")\n",
    "        print(f\"Content length: {len(result.content)}\")\n",
    "        print(\"\\nFirst 200 characters:\")\n",
    "        print(result.content[:200] + \"...\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Run test\n",
    "await test_single_url()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7724d6-f9d3-46f1-8274-d86dca83b14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RAG process\n",
    "async def test_rag_process():\n",
    "    print(\"\\nüîç Testing RAG Processing...\")\n",
    "    \n",
    "    # 1. First get the article content\n",
    "    print(\"1. Getting article content...\")\n",
    "    scraper = WebScraper(jina_api_key)\n",
    "    url = \"https://www.anthropic.com/research/building-effective-agents\"\n",
    "    article = await scraper.scrape(url)\n",
    "    print(f\"‚úì Article scraped, length: {len(article.content)}\")\n",
    "    \n",
    "    # 2. Initialize RAG processor\n",
    "    print(\"\\n2. Initializing RAG processor...\")\n",
    "    rag = RagProcessor(\n",
    "        openai_key=openai_key,\n",
    "        pinecone_key=pinecone_key,\n",
    "        index_name=\"augmentedllm\" \n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # 3. Store article\n",
    "        print(\"\\n3. Processing and storing article...\")\n",
    "        vector_id = await rag.store_article(article)\n",
    "        print(f\"‚úì Article stored with first chunk ID: {vector_id}\")\n",
    "        \n",
    "        # 4. Test retrieval\n",
    "        print(\"\\n4. Testing retrieval...\")\n",
    "        query = \"What are the key patterns for building effective agents?\"\n",
    "        matches = await rag.get_context(query)\n",
    "        \n",
    "        print(\"\\nRetrieved Contexts:\")\n",
    "        for i, match in enumerate(matches, 1):\n",
    "            print(f\"\\nMatch {i} (Score: {match['score']:.3f}):\")\n",
    "            print(f\"Text: {match['metadata']['text'][:200]}...\")\n",
    "        \n",
    "        return matches\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Run test\n",
    "matches = await test_rag_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd69b57-0e43-494b-8827-13f30dd7dbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test full pipeline\n",
    "async def test_full_pipeline():\n",
    "    print(\"\\nüöÄ Testing Full Pipeline\")\n",
    "    \n",
    "    # Initialize pipeline\n",
    "    pipeline = ContentPipeline(\n",
    "        openai_key=openai_key,\n",
    "        jina_key=jina_api_key,\n",
    "        pinecone_key=pinecone_key,\n",
    "        index_name=\"augmentedllm\"\n",
    "    )\n",
    "    \n",
    "    # Test URL\n",
    "    url = \"https://www.anthropic.com/research/building-effective-agents\"\n",
    "    \n",
    "    try:\n",
    "        print(\"\\n1Ô∏è‚É£ Processing URL...\")\n",
    "        final_post = await pipeline.process_url(url)\n",
    "        \n",
    "        print(\"\\nüìä Results:\")\n",
    "        \n",
    "        print(\"\\nüéØ Available Languages:\")\n",
    "        for lang, content in final_post.content.items():\n",
    "            print(f\"\\n{lang.upper()}:\")\n",
    "            print(f\"Length: {len(content)} characters\")\n",
    "            print(\"Preview:\")\n",
    "            print(content[:200] + \"...\")\n",
    "        \n",
    "        print(\"\\n#Ô∏è‚É£ Hashtags:\")\n",
    "        print(\", \".join(final_post.hashtags))\n",
    "        \n",
    "        print(\"\\nüí° Platform Versions:\")\n",
    "        for platform, content in final_post.platform_specific.items():\n",
    "            print(f\"\\n{platform.upper()}:\")\n",
    "            print(f\"Preview: {content['text'][:100]}...\")\n",
    "        \n",
    "        print(\"\\nüìà Engagement Metrics:\")\n",
    "        for metric, value in final_post.engagement_metrics.items():\n",
    "            print(f\"- {metric}: {value}\")\n",
    "        \n",
    "        return final_post\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Run the test\n",
    "result = await test_full_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a9c5bf-0e6e-476d-8a4f-4d93b69bba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40057ed0-00ac-470d-8b24-acc4bea92886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ContentAnalyst\n",
    "async def test_content_analyst():\n",
    "    print(\"\\nüîç Testing ContentAnalyst...\")\n",
    "    \n",
    "    # 1. Get article content\n",
    "    print(\"1. Getting article...\")\n",
    "    scraper = WebScraper(jina_api_key)\n",
    "    article = await scraper.scrape(\"https://www.anthropic.com/research/building-effective-agents\")\n",
    "    print(f\"‚úì Got article, length: {len(article.content)}\")\n",
    "    \n",
    "    # 2. Process with analyst\n",
    "    print(\"\\n2. Generating brief...\")\n",
    "    analyst = ContentAnalyst(\n",
    "        api_key=openai_key,\n",
    "        rag_processor=RagProcessor(openai_key, pinecone_key, \"augmentedllm\")\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        brief = await analyst.process(article)\n",
    "        \n",
    "        print(\"\\nüìã Content Brief:\")\n",
    "        print(f\"Main topics: {brief.main_topics}\")\n",
    "        print(f\"Target audience: {brief.target_audience}\")\n",
    "        print(f\"Key messages: {brief.key_messages}\")\n",
    "        print(f\"Tone: {brief.tone_guidelines}\")\n",
    "        print(f\"Hashtags: {brief.hashtags}\")\n",
    "        \n",
    "        return brief\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Run test\n",
    "brief = await test_content_analyst()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbe3d04-fb60-4210-ad60-fac94f0421c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ContentWriter\n",
    "async def test_content_writer():\n",
    "    print(\"\\nüîç Testing Content Writer...\")\n",
    "    \n",
    "    # 1. First get the brief\n",
    "    print(\"1. Getting brief...\")\n",
    "    brief = await test_content_analyst()\n",
    "    print(\"‚úì Got brief\")\n",
    "    \n",
    "    # 2. Generate post\n",
    "    print(\"\\n2. Creating post draft...\")\n",
    "    writer = ContentWriter(\n",
    "        api_key=openai_key,\n",
    "        rag_processor=RagProcessor(openai_key, pinecone_key, \"augmentedllm\")\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        draft = await writer.process(brief)\n",
    "        \n",
    "        print(\"\\nüìù Post Draft:\")\n",
    "        print(f\"Content: {draft.content}\")\n",
    "        print(f\"Hashtags: {draft.hashtags}\")\n",
    "        \n",
    "        return draft\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Run test\n",
    "draft = await test_content_writer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2c22b5-3e62-4406-93c8-c0a9de081a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ContentOptimizer\n",
    "async def test_content_optimizer():\n",
    "    print(\"\\nüîç Testing Content Optimizer...\")\n",
    "    \n",
    "    # 1. First get the draft\n",
    "    print(\"1. Getting draft...\")\n",
    "    draft = await test_content_writer()\n",
    "    print(\"‚úì Got draft\")\n",
    "    \n",
    "    # 2. Optimize post\n",
    "    print(\"\\n2. Optimizing post...\")\n",
    "    optimizer = ContentOptimizer(openai_key)\n",
    "    \n",
    "    try:\n",
    "        final = await optimizer.process(draft)\n",
    "        \n",
    "        print(\"\\n‚ú® Final Post:\")\n",
    "        for lang, text in final.content.items():\n",
    "            print(f\"\\n{lang.upper()}:\")\n",
    "            print(text)\n",
    "        \n",
    "        print(\"\\nüìä Expected Engagement:\")\n",
    "        for metric, value in final.engagement_metrics.items():\n",
    "            print(f\"{metric}: {value}\")\n",
    "            \n",
    "        print(\"\\nüì± Platform Versions:\")\n",
    "        for platform, text in final.platform_specific.items():\n",
    "            print(f\"\\n{platform.upper()}:\")\n",
    "            print(text)\n",
    "        \n",
    "        return final\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Run full pipeline test\n",
    "final = await test_content_optimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88460416-9014-486c-a9d5-cdda3eb7d5cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
