{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbcfa351-3a29-41fd-b654-4d4fade4200f",
   "metadata": {},
   "source": [
    "# Building an Augmented LLM Agent\n",
    "This notebook demonstrates the evolution of a LLM agent from a simple chatbot to a sophisticated system with memory, retrieval, and tool use capabilities. We'll follow the approach outlined in Anthropic's \"Building Effective Agents\" article, implementing each capability step by step.\n",
    "\n",
    "Key concepts we'll cover:\n",
    "\n",
    "- Basic LLM interactions\n",
    "- Adding conversation memory\n",
    "- Implementing RAG (Retrieval Augmented Generation)\n",
    "- Integrating specialized tools\n",
    "\n",
    "Each section builds upon the previous one, demonstrating how we can progressively enhance our LLM's capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9933b1-7bc1-4cd2-a5ad-5d3ef3337c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from typing import List, Dict, Optional\n",
    "import json\n",
    "import requests\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded248de-861d-4773-8839-100cd3f77f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "index_name = \"augmentedllm\"\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "_set_env(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210d06aa-a8a9-4a90-a13a-4a49cfc78b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper class for running dialogue tests\n",
    "class DialogueTest:\n",
    "    \"\"\"Simple framework for testing LLM dialogue capabilities.\"\"\"\n",
    "    \n",
    "    def __init__(self, agent):\n",
    "        \"\"\"Initialize with an LLM agent to test.\"\"\"\n",
    "        self.agent = agent\n",
    "    \n",
    "    def chat(self, message: str) -> str:\n",
    "        \"\"\"Send a message and get the response.\"\"\"\n",
    "        print(f\"\\nUser: {message}\")\n",
    "        response = self.agent.process_message(message)\n",
    "        print(f\"Assistant: {response}\")\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c70d729-5096-40bf-9f4f-2be75da6ff43",
   "metadata": {},
   "source": [
    "## Part 1: Basic LLM Implementation\n",
    "In this first step, we create a simple chatbot that can interact with OpenAI's API. This basic implementation:\n",
    "\n",
    "- Makes individual API calls for each message\n",
    "- Has no memory of previous interactions\n",
    "- Uses a simple system prompt\n",
    "\n",
    "Key Points:\n",
    "- Each interaction is independent\n",
    "- No context preservation between messages\n",
    "- Demonstrates bare minimum for LLM interaction\n",
    "\n",
    "Let's see how this basic version works and why we need more sophisticated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f30281d-b8ca-4995-bb5d-3494b6376f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicLLM:\n",
    "    \"\"\"Basic LLM with just chat capability.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, model: str = \"gpt-4o-mini\"):\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "        \n",
    "    def process_message(self, message: str) -> str:\n",
    "        \"\"\"Process a single message.\"\"\"\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                \"https://api.openai.com/v1/chat/completions\",\n",
    "                headers={\n",
    "                    \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "                    \"Content-Type\": \"application/json\"\n",
    "                },\n",
    "                json={\n",
    "                    \"model\": self.model,\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": \"You are a helpful AI assistant.\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": message\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response.json()['choices'][0]['message']['content']\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feb2884-e179-4297-a2e2-6f3295ff85d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Basic LLM\n",
    "basic_llm = BasicLLM(os.environ[\"OPENAI_API_KEY\"])\n",
    "basic_test = DialogueTest(basic_llm)\n",
    "\n",
    "print(\"\\nTesting Basic LLM:\")\n",
    "basic_test.chat(\"Hi! My name is Bob.\")\n",
    "basic_test.chat(\"Do you remember my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadefd1a-5540-44e2-a1ce-65376067a256",
   "metadata": {},
   "source": [
    "## Part 2: Adding Conversation Memory\n",
    "\n",
    "Now we enhance our LLM with conversation memory. This allows the agent to:\n",
    "- Remember previous interactions\n",
    "- Maintain context throughout a conversation\n",
    "- Provide more coherent and contextual responses\n",
    "\n",
    "Key Improvements:\n",
    "- Stores conversation history\n",
    "- Includes past messages in new requests\n",
    "- Uses Pydantic for structured data management\n",
    "\n",
    "This step significantly improves the agent's ability to maintain meaningful conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e14c228-c484-4369-ab6a-56916d50b94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Message(BaseModel):\n",
    "    \"\"\"Single message in conversation history.\"\"\"\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "class MemoryLLM(BasicLLM):\n",
    "    \"\"\"LLM with conversation memory.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, model: str = \"gpt-4o-mini\"):\n",
    "        super().__init__(api_key, model)\n",
    "        self.history: List[Message] = []\n",
    "        \n",
    "    def process_message(self, message: str) -> str:\n",
    "        \"\"\"Process message with conversation history.\"\"\"\n",
    "        try:\n",
    "            # Prepare messages including history\n",
    "            messages = [{\"role\": \"system\", \"content\": \"You are a helpful marketing assistant. Answer just if you sure about correct info 100%. If not say you not sure.\"}]\n",
    "            messages.extend([msg.dict() for msg in self.history])\n",
    "            messages.append({\"role\": \"user\", \"content\": message})\n",
    "            \n",
    "            response = requests.post(\n",
    "                \"https://api.openai.com/v1/chat/completions\",\n",
    "                headers={\n",
    "                    \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "                    \"Content-Type\": \"application/json\"\n",
    "                },\n",
    "                json={\n",
    "                    \"model\": self.model,\n",
    "                    \"messages\": messages\n",
    "                }\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Get response and update history\n",
    "            assistant_message = response.json()['choices'][0]['message']['content']\n",
    "            self.history.append(Message(role=\"user\", content=message))\n",
    "            self.history.append(Message(role=\"assistant\", content=assistant_message))\n",
    "            \n",
    "            return assistant_message\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2be4e9-5448-4f92-a938-f0244b16325d",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Test 2: LLM with Memory\n",
    "print(\"\\n=== Testing LLM with Memory ===\")\n",
    "memory_llm = MemoryLLM(os.environ[\"OPENAI_API_KEY\"])\n",
    "memory_test = DialogueTest(memory_llm)\n",
    "memory_test.chat(\"Hi! My name is Bob.\")\n",
    "memory_test.chat(\"Do you remember my name?\") \n",
    "memory_test.chat(\"What do you khow about ...?\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f978d648-7651-41f1-9d03-92e772062995",
   "metadata": {},
   "source": [
    "## Part 3a: Implementing RAG - Document Processing\n",
    "\n",
    "RAG (Retrieval Augmented Generation) enhances our LLM with the ability to use external knowledge. This section focuses on processing and storing documents:\n",
    "\n",
    "Key Components:\n",
    "1. Document Chunking: Breaking documents into manageable pieces\n",
    "2. Embedding Generation: Converting text to vector representations\n",
    "3. Vector Storage: Storing embeddings in Pinecone for efficient retrieval\n",
    "\n",
    "This step sets up the infrastructure for knowledge-enhanced responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df9ac96-3dbd-439d-bc99-f9ef5b626704",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    \"\"\"Processes documents into chunks.\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "\n",
    "    def create_chunks(self, text: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Split text into overlapping chunks.\"\"\"\n",
    "        if not text:\n",
    "            raise ValueError(\"Empty text provided\")\n",
    "        if self.chunk_size <= self.chunk_overlap:\n",
    "            raise ValueError(\"chunk_size must be greater than chunk_overlap\")\n",
    "        \n",
    "        chunks = []\n",
    "        start = 0\n",
    "        \n",
    "        while start < len(text):\n",
    "            end = start + self.chunk_size\n",
    "            chunk = text[start:end]\n",
    "            \n",
    "            chunks.append({\n",
    "                'text': chunk,\n",
    "                'metadata': {'start_char': start, 'end_char': end}\n",
    "            })\n",
    "            \n",
    "            start += self.chunk_size - self.chunk_overlap\n",
    "            \n",
    "        print(f\"\\nCreated {len(chunks)} chunks\")\n",
    "        print(f\"Sample chunk (first 50 chars): {chunks[0]['text'][:50]}...\")\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5193586b-c6be-446e-a586-20e28c416976",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingGenerator:\n",
    "    \"\"\"Handles text embedding generation.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.api_url = \"https://api.openai.com/v1/embeddings\"\n",
    "        \n",
    "    def create_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"Generate embedding for text.\"\"\"\n",
    "        if not text.strip():\n",
    "            raise ValueError(\"Empty text provided\")\n",
    "            \n",
    "        response = requests.post(\n",
    "            self.api_url,\n",
    "            headers={\n",
    "                \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            },\n",
    "            json={\n",
    "                \"model\": \"text-embedding-3-small\",\n",
    "                \"input\": text\n",
    "            }\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        embedding = response.json()['data'][0]['embedding']\n",
    "        print(f\"Generated embedding, sample of first 3 dimensions: {embedding[:3]}\")\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dff8ac-9492-498e-be39-d8876c252141",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages vector storage operations.\"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str, index_name: str):\n",
    "        self.api_key = api_key\n",
    "        self.index_name = index_name\n",
    "        self.headers = {\n",
    "            \"Api-Key\": self.api_key,\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"X-Pinecone-API-Version\": \"2024-07\"\n",
    "        }\n",
    "        self.index_host_cache = {}\n",
    "        \n",
    "    def describe_index(self) -> str:\n",
    "        \"\"\"Get or retrieve cached index host.\"\"\"\n",
    "        if self.index_name in self.index_host_cache:\n",
    "            return self.index_host_cache[self.index_name]\n",
    "            \n",
    "        url = f\"https://api.pinecone.io/indexes/{self.index_name}\"\n",
    "        response = requests.get(url, headers=self.headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        host = response.json()[\"host\"]\n",
    "        self.index_host_cache[self.index_name] = host\n",
    "        print(f\"\\nConnected to Pinecone host: {host}\")\n",
    "        return host\n",
    "        \n",
    "    def query_vectors(self, \n",
    "                      query_vector: List[float], \n",
    "                      top_k: int = 3, \n",
    "                      namespace: str = \"\") -> List[Dict]:\n",
    "        \"\"\"Query for most similar vectors.\"\"\"\n",
    "        host = self.describe_index()\n",
    "        url = f\"https://{host}/query\"\n",
    "        data = {\n",
    "            \"vector\": query_vector,\n",
    "            \"topK\": top_k,\n",
    "            \"namespace\": namespace,\n",
    "            \"includeMetadata\": True\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nQuerying Pinecone for top {top_k} matches\")\n",
    "        response = requests.post(url, headers=self.headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        matches = response.json()['matches']\n",
    "        return matches\n",
    "        \n",
    "    def store_vectors(self, vectors: List[Dict], namespace: str = \"\") -> Dict:\n",
    "        \"\"\"Store vectors in Pinecone.\"\"\"\n",
    "        host = self.describe_index()\n",
    "        url = f\"https://{host}/vectors/upsert\"\n",
    "        data = {\n",
    "            \"vectors\": vectors,\n",
    "            \"namespace\": namespace\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nUploading {len(vectors)} vectors to Pinecone\")\n",
    "        print(f\"Sample vector metadata: {vectors[0]['metadata']}\")\n",
    "        \n",
    "        response = requests.post(url, headers=self.headers, json=data)    \n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        print(f\"Pinecone response: {result}\")\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5470878-e8d8-4477-9155-14737be2d8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_store_document(text: str, document_id: str, openai_key: str, pinecone_key: str, index_name: str) -> bool:\n",
    "    \"\"\"Process document and store in vector database.\"\"\"\n",
    "    try:\n",
    "        # Initialize components\n",
    "        processor = DocumentProcessor()\n",
    "        embedding_gen = EmbeddingGenerator(openai_key)\n",
    "        vector_store = VectorStore(pinecone_key, index_name)\n",
    "        \n",
    "        # Create chunks\n",
    "        chunks = processor.create_chunks(text)\n",
    "        \n",
    "        # Generate embeddings and prepare vectors\n",
    "        vectors = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            embedding = embedding_gen.create_embedding(chunk['text'])\n",
    "            if embedding:\n",
    "                vectors.append({\n",
    "                    'id': f\"{document_id}-{i}\",\n",
    "                    'values': embedding,\n",
    "                    'metadata': {\n",
    "                        'text': chunk['text'],\n",
    "                        'document_id': document_id,\n",
    "                        **chunk['metadata']\n",
    "                    }\n",
    "                })\n",
    "        \n",
    "        # Store vectors\n",
    "        if vectors:\n",
    "            vector_store.store_vectors(vectors)\n",
    "            return True\n",
    "        return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing document: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae50ac4-4b2f-42e0-848d-fff7a12c5177",
   "metadata": {},
   "source": [
    "## Part 3b: RAG-Enabled LLM\n",
    "\n",
    "Now we integrate the RAG capabilities with our LLM. This combination allows the agent to:\n",
    "- Search for relevant information in our document base\n",
    "- Include retrieved context in its responses\n",
    "- Provide more informed and accurate answers\n",
    "\n",
    "The key enhancement is the ability to ground responses in specific knowledge rather "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90e1700-b9d5-4dc3-8198-130666925c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RagLLM(MemoryLLM):\n",
    "    \"\"\"LLM with conversation memory and RAG capability.\"\"\"\n",
    "    \n",
    "    def __init__(self, openai_key: str, pinecone_key: str, index_name: str, model: str = \"gpt-4o-mini\"):\n",
    "        super().__init__(openai_key, model)\n",
    "        self.pinecone_key = pinecone_key\n",
    "        self.index_name = index_name\n",
    "        self.embedding_generator = EmbeddingGenerator(openai_key)\n",
    "        self.vector_store = VectorStore(pinecone_key, index_name)\n",
    "        \n",
    "    def _get_relevant_chunks(self, text: str, top_k: int = 3) -> List[str]:\n",
    "        \"\"\"Get relevant text chunks using vector search.\"\"\"\n",
    "        try:\n",
    "            # Get query embedding\n",
    "            query_vector = self.embedding_generator.create_embedding(text)\n",
    "            \n",
    "            # Query vector store\n",
    "            matches = self.vector_store.query_vectors(\n",
    "                query_vector=query_vector,\n",
    "                top_k=top_k\n",
    "            )\n",
    "            \n",
    "            # Extract texts from metadata\n",
    "            return [match['metadata'].get('text', '') for match in matches]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting relevant chunks: {e}\")\n",
    "            return []\n",
    "            \n",
    "    def process_message(self, message: str) -> str:\n",
    "        \"\"\"Process message with conversation history and relevant context.\"\"\"\n",
    "        try:\n",
    "            # Get relevant context\n",
    "            context_chunks = self._get_relevant_chunks(message)\n",
    "            context = \"\\n\".join(context_chunks)\n",
    "            print(f\"\\nFound {len(context_chunks)} relevant chunks\")\n",
    "            if context_chunks:\n",
    "                print(f\"Sample context (first 50 chars): {context_chunks[0][:50]}...\")\n",
    "            \n",
    "            # Prepare messages\n",
    "            messages = [{\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"\"\"You are a helpful marketing assistant. Answer just if you sure about correct info 100%. If not say you not sure. \n",
    "                Use this context when relevant:\n",
    "                {context}\n",
    "                \"\"\"\n",
    "            }]\n",
    "            messages.extend([msg.dict() for msg in self.history])\n",
    "            messages.append({\"role\": \"user\", \"content\": message})\n",
    "            \n",
    "            response = requests.post(\n",
    "                \"https://api.openai.com/v1/chat/completions\",\n",
    "                headers={\n",
    "                    \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "                    \"Content-Type\": \"application/json\"\n",
    "                },\n",
    "                json={\n",
    "                    \"model\": self.model,\n",
    "                    \"messages\": messages\n",
    "                }\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Get response and update history\n",
    "            assistant_message = response.json()['choices'][0]['message']['content']\n",
    "            self.history.append(Message(role=\"user\", content=message))\n",
    "            self.history.append(Message(role=\"assistant\", content=assistant_message))\n",
    "            \n",
    "            return assistant_message\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1621347b-5cab-4b7e-8c0b-694ecf3d3849",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Testing Document Processing ===\")\n",
    "with open(\"YOUR_DOC_NAME.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "    success = process_and_store_document(\n",
    "        text=text,\n",
    "        document_id=\"DOC_NAME\",\n",
    "        openai_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "        pinecone_key=os.environ[\"PINECONE_API_KEY\"],\n",
    "        index_name=\"augmentedllm\"\n",
    "    )\n",
    "    print(f\"\\nDocument processing {'succeeded' if success else 'failed'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a4fbec-928b-4d2e-9693-3b51e5104013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3b: LLM with Memory and RAG\n",
    "print(\"\\n=== Testing LLM with Memory and RAG ===\")\n",
    "rag_llm = RagLLM(os.environ[\"OPENAI_API_KEY\"], os.environ[\"PINECONE_API_KEY\"], \"augmentedllm\")\n",
    "rag_test = DialogueTest(rag_llm)\n",
    "rag_test.chat(\"Hi! My name is Bob.\")\n",
    "rag_test.chat(\"Do you remember my name?\")\n",
    "rag_test.chat(\"What do you khow about ...?\")\n",
    "rag_test.chat(\"We spent $10,000 on a marketing campaign that generated $50,000 in revenue with a 10% margin. What was our ROMI?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18337fb-5791-4ac0-9145-a257e843015f",
   "metadata": {},
   "source": [
    "## Part 4: Adding Specialized Tools\n",
    "\n",
    "In our final enhancement, we add specialized tools to our agent. This example implements marketing analytics tools, specifically ROMI calculation.\n",
    "\n",
    "Key Features:\n",
    "- Tool definition system\n",
    "- Automatic tool selection by the LLM\n",
    "- Result integration into responses\n",
    "\n",
    "This demonstrates how we can extend our agent with specific computational capabiliti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e55867b-3458-4d59-964d-26255c9b13c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketingTools:\n",
    "    \"\"\"Provides marketing analytics calculations.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_tool_definitions() -> List[Dict]:\n",
    "        return [{\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"calculate_romi\",\n",
    "                \"description\": \"Calculate Return on Marketing Investment (ROMI)\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"revenue\": {\n",
    "                            \"type\": \"number\",\n",
    "                            \"description\": \"Total revenue generated from marketing campaign\"\n",
    "                        },\n",
    "                        \"marketing_cost\": {\n",
    "                            \"type\": \"number\",\n",
    "                            \"description\": \"Total cost of marketing campaign\"\n",
    "                        },\n",
    "                        \"margin_percent\": {\n",
    "                            \"type\": \"number\",\n",
    "                            \"description\": \"Profit margin percentage (0-100)\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"revenue\", \"marketing_cost\", \"margin_percent\"]\n",
    "                }\n",
    "            }\n",
    "        }]\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_romi(revenue: float, marketing_cost: float, margin_percent: float) -> float:\n",
    "        \"\"\"\n",
    "        Calculate ROMI using the formula: ((Revenue * Margin%) - Marketing Cost) / Marketing Cost * 100\n",
    "        Returns percentage value\n",
    "        \"\"\"\n",
    "        if marketing_cost <= 0:\n",
    "            raise ValueError(\"Marketing cost must be greater than zero\")\n",
    "        if not (0 <= margin_percent <= 100):\n",
    "            raise ValueError(\"Margin percentage must be between 0 and 100\")\n",
    "            \n",
    "        margin_multiplier = margin_percent / 100\n",
    "        profit = revenue * margin_multiplier - marketing_cost\n",
    "        romi = (profit / marketing_cost) * 100\n",
    "        return round(romi, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be382ae4-8e33-4d29-9696-af291a397a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolEnabledLLM(RagLLM):\n",
    "    \"\"\"LLM with memory, RAG and tool usage capability.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 openai_key: str, \n",
    "                 pinecone_key: str, \n",
    "                 index_name: str, \n",
    "                 model: str = \"gpt-4o-mini\"):\n",
    "        super().__init__(openai_key, pinecone_key, index_name, model)\n",
    "        self.tools = MarketingTools()\n",
    "    \n",
    "    def process_message(self, message: str) -> str:\n",
    "        \"\"\"Process message with tools, memory and RAG.\"\"\"\n",
    "        try:\n",
    "            # Get relevant context\n",
    "            context_chunks = self._get_relevant_chunks(message)\n",
    "            context = \"\\n\".join(context_chunks)\n",
    "            print(f\"\\nFound {len(context_chunks)} relevant chunks\")\n",
    "            if context_chunks:\n",
    "                print(f\"Sample context (first 100 chars): {context_chunks[0][:100]}...\")\n",
    "            \n",
    "            # Prepare messages\n",
    "            messages = [{\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"\"\"You are a marketing analytics assistant. \n",
    "                Use this context when relevant:\n",
    "                {context}\n",
    "                \n",
    "                When asked about marketing metrics, use the calculate_romi tool.\n",
    "                ROMI (Return on Marketing Investment) shows the profitability of marketing spending.\n",
    "                \"\"\"\n",
    "            }]\n",
    "            messages.extend([msg.dict() for msg in self.history])\n",
    "            messages.append({\"role\": \"user\", \"content\": message})\n",
    "            \n",
    "            # Make API request with tools\n",
    "            response = requests.post(\n",
    "                \"https://api.openai.com/v1/chat/completions\",\n",
    "                headers={\n",
    "                    \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "                    \"Content-Type\": \"application/json\"\n",
    "                },\n",
    "                json={\n",
    "                    \"model\": self.model,\n",
    "                    \"messages\": messages,\n",
    "                    \"tools\": self.tools.get_tool_definitions()\n",
    "                }\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Process response and tool calls\n",
    "            response_data = response.json()\n",
    "            assistant_message = response_data['choices'][0]['message']\n",
    "            \n",
    "            if tool_calls := assistant_message.get('tool_calls'):\n",
    "                print(\"\\nProcessing tool calls...\")\n",
    "                tool_results = self._handle_tool_calls(tool_calls)\n",
    "                \n",
    "                # Add tool results to conversation\n",
    "                messages.extend([\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": assistant_message.get('content'),\n",
    "                        \"tool_calls\": tool_calls\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"tool\",\n",
    "                        \"content\": json.dumps(tool_results),\n",
    "                        \"tool_call_id\": tool_calls[0]['id']\n",
    "                    }\n",
    "                ])\n",
    "                \n",
    "                # Get final response with tool results\n",
    "                final_response = requests.post(\n",
    "                    \"https://api.openai.com/v1/chat/completions\",\n",
    "                    headers={\n",
    "                        \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "                        \"Content-Type\": \"application/json\"\n",
    "                    },\n",
    "                    json={\n",
    "                        \"model\": self.model,\n",
    "                        \"messages\": messages\n",
    "                    }\n",
    "                )\n",
    "                final_response.raise_for_status()\n",
    "                assistant_message = final_response.json()['choices'][0]['message']\n",
    "            \n",
    "            # Update conversation history\n",
    "            self.history.append(Message(role=\"user\", content=message))\n",
    "            self.history.append(Message(role=\"assistant\", content=assistant_message['content']))\n",
    "            \n",
    "            return assistant_message['content']\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "            \n",
    "    def _handle_tool_calls(self, tool_calls: List[Dict]) -> Dict:\n",
    "        \"\"\"Process tool calls and return results.\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for call in tool_calls:\n",
    "            if call['function']['name'] == 'calculate_romi':\n",
    "                try:\n",
    "                    args = json.loads(call['function']['arguments'])\n",
    "                    result = self.tools.calculate_romi(\n",
    "                        revenue=args['revenue'],\n",
    "                        marketing_cost=args['marketing_cost'],\n",
    "                        margin_percent=args['margin_percent']\n",
    "                    )\n",
    "                    print(f\"Calculated ROMI: {result}%\")\n",
    "                    results[call['id']] = result\n",
    "                except Exception as e:\n",
    "                    results[call['id']] = f\"Error calculating ROMI: {str(e)}\"\n",
    "                    \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1de42b0-5397-4a07-ab20-cf345ebb14c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: LLM with Tools\n",
    "print(\"\\n=== Testing Tool-enabled LLM ===\")\n",
    "tool_llm = ToolEnabledLLM(os.environ[\"OPENAI_API_KEY\"], os.environ[\"PINECONE_API_KEY\"], \"augmentedllm\")\n",
    "tool_test = DialogueTest(tool_llm)\n",
    "tool_test.chat(\"Hi! My name is Bob?\")\n",
    "tool_test.chat(\"Do you remember my name?\")\n",
    "tool_test.chat(\"How will we ...?\")\n",
    "tool_test.chat(\"We spent $10,000 on a marketing campaign that generated $50,000 in revenue with a 10% margin. What was our ROMI?\")\n",
    "tool_test.chat(\"How about a campaign where we spent $200,000 and got $4,000,000 in revenue with 50% margin?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
