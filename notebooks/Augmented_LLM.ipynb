{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb9933b1-7bc1-4cd2-a5ad-5d3ef3337c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from typing import List, Dict, Optional\n",
    "import json\n",
    "import requests\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ded248de-861d-4773-8839-100cd3f77f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY:  ········\n",
      "PINECONE_API_KEY:  ········\n"
     ]
    }
   ],
   "source": [
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "index_name = \"augmentedllm\"\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "_set_env(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "210d06aa-a8a9-4a90-a13a-4a49cfc78b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper class for running dialogue tests\n",
    "class DialogueTest:\n",
    "    \"\"\"Simple framework for testing LLM dialogue capabilities.\"\"\"\n",
    "    \n",
    "    def __init__(self, agent):\n",
    "        \"\"\"Initialize with an LLM agent to test.\"\"\"\n",
    "        self.agent = agent\n",
    "    \n",
    "    def chat(self, message: str) -> str:\n",
    "        \"\"\"Send a message and get the response.\"\"\"\n",
    "        print(f\"\\nUser: {message}\")\n",
    "        response = self.agent.process_message(message)\n",
    "        print(f\"Assistant: {response}\")\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9c9b5e8-8ea3-4b8a-ac30-1a6a25113ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Part 1: Basic LLM ===\n",
      "Simple LLM that can chat but doesn't remember context\n"
     ]
    }
   ],
   "source": [
    "# Part 1: Basic LLM Implementation\n",
    "print(\"\\n=== Part 1: Basic LLM ===\")\n",
    "print(\"Simple LLM that can chat but doesn't remember context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f30281d-b8ca-4995-bb5d-3494b6376f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicLLM:\n",
    "    \"\"\"Basic LLM with just chat capability.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, model: str = \"gpt-4o-mini\"):\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "        \n",
    "    def process_message(self, message: str) -> str:\n",
    "        \"\"\"Process a single message.\"\"\"\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                \"https://api.openai.com/v1/chat/completions\",\n",
    "                headers={\n",
    "                    \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "                    \"Content-Type\": \"application/json\"\n",
    "                },\n",
    "                json={\n",
    "                    \"model\": self.model,\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": \"You are a helpful AI assistant.\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": message\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response.json()['choices'][0]['message']['content']\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3feb2884-e179-4297-a2e2-6f3295ff85d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Basic LLM:\n",
      "\n",
      "User: Hi! My name is Alice.\n",
      "Assistant: Hi Alice! How can I assist you today?\n",
      "\n",
      "User: Do you remember my name?\n",
      "Assistant: I don’t have the ability to remember personal details or past interactions, including names. How can I assist you today?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I don’t have the ability to remember personal details or past interactions, including names. How can I assist you today?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Basic LLM\n",
    "basic_llm = BasicLLM(os.environ[\"OPENAI_API_KEY\"])\n",
    "basic_test = DialogueTest(basic_llm)\n",
    "\n",
    "print(\"\\nTesting Basic LLM:\")\n",
    "basic_test.chat(\"Hi! My name is Viacheslav.\")\n",
    "basic_test.chat(\"Do you remember my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0194573-3a7b-4b55-9e1f-ae500d3d94db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86d10d9e-effb-4789-a150-6101ad6f604f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Part 2: LLM with Memory ===\n",
      "Now our LLM can remember conversation context\n"
     ]
    }
   ],
   "source": [
    "# Part 2: LLM with Memory\n",
    "print(\"\\n=== Part 2: LLM with Memory ===\")\n",
    "print(\"Now our LLM can remember conversation context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e14c228-c484-4369-ab6a-56916d50b94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Message(BaseModel):\n",
    "    \"\"\"Single message in conversation history.\"\"\"\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "class MemoryLLM(BasicLLM):\n",
    "    \"\"\"LLM with conversation memory.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, model: str = \"gpt-4o-mini\"):\n",
    "        super().__init__(api_key, model)\n",
    "        self.history: List[Message] = []\n",
    "        \n",
    "    def process_message(self, message: str) -> str:\n",
    "        \"\"\"Process message with conversation history.\"\"\"\n",
    "        try:\n",
    "            # Prepare messages including history\n",
    "            messages = [{\"role\": \"system\", \"content\": \"You are a helpful marketing assistant. Answer just if you sure about correct info 100%. If not say you not sure.\"}]\n",
    "            messages.extend([msg.dict() for msg in self.history])\n",
    "            messages.append({\"role\": \"user\", \"content\": message})\n",
    "            \n",
    "            response = requests.post(\n",
    "                \"https://api.openai.com/v1/chat/completions\",\n",
    "                headers={\n",
    "                    \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "                    \"Content-Type\": \"application/json\"\n",
    "                },\n",
    "                json={\n",
    "                    \"model\": self.model,\n",
    "                    \"messages\": messages\n",
    "                }\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Get response and update history\n",
    "            assistant_message = response.json()['choices'][0]['message']['content']\n",
    "            self.history.append(Message(role=\"user\", content=message))\n",
    "            self.history.append(Message(role=\"assistant\", content=assistant_message))\n",
    "            \n",
    "            return assistant_message\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac2be4e9-5448-4f92-a938-f0244b16325d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing LLM with Memory ===\n",
      "\n",
      "User: Hi! My name is Viacheslav.\n",
      "Assistant: Hello Viacheslav! How can I assist you today?\n",
      "\n",
      "User: Do you remember my name?\n",
      "Assistant: Yes, your name is Viacheslav. How can I help you today?\n",
      "\n",
      "User: What you khow about The Global Sourcing RFI Company?\n",
      "Assistant: I'm not sure about specific current details regarding The Global Sourcing RFI Company, as my training data only goes up to October 2023, and I don't have direct knowledge about every company. If you need information about their services, reputation, or other specific inquiries, I recommend checking their official website or recent news articles for the most accurate and up-to-date information.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm not sure about specific current details regarding The Global Sourcing RFI Company, as my training data only goes up to October 2023, and I don't have direct knowledge about every company. If you need information about their services, reputation, or other specific inquiries, I recommend checking their official website or recent news articles for the most accurate and up-to-date information.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Test 2: LLM with Memory\n",
    "print(\"\\n=== Testing LLM with Memory ===\")\n",
    "memory_llm = MemoryLLM(os.environ[\"OPENAI_API_KEY\"])\n",
    "memory_test = DialogueTest(memory_llm)\n",
    "memory_test.chat(\"Hi! My name is Viacheslav.\")\n",
    "memory_test.chat(\"Do you remember my name?\") \n",
    "memory_test.chat(\"What you khow about The Global Sourcing RFI Company?\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4cba51-3f66-43ee-bbf4-54f5e7ef8857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3aa7f09-6a1b-437d-baf8-34d6a4ba523e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Part 3a: Document Processing and Indexing ===\n",
      "First step of RAG: Processing and storing documents\n"
     ]
    }
   ],
   "source": [
    "# Part 3a: Document Processing and Indexing\n",
    "print(\"\\n=== Part 3a: Document Processing and Indexing ===\")\n",
    "print(\"First step of RAG: Processing and storing documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf75e5a3-50be-4ad2-b02e-767a8eb90642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5df9ac96-3dbd-439d-bc99-f9ef5b626704",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    \"\"\"Processes documents into chunks.\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "\n",
    "    def create_chunks(self, text: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Split text into overlapping chunks.\"\"\"\n",
    "        if not text:\n",
    "            raise ValueError(\"Empty text provided\")\n",
    "        if self.chunk_size <= self.chunk_overlap:\n",
    "            raise ValueError(\"chunk_size must be greater than chunk_overlap\")\n",
    "        \n",
    "        chunks = []\n",
    "        start = 0\n",
    "        \n",
    "        while start < len(text):\n",
    "            end = start + self.chunk_size\n",
    "            chunk = text[start:end]\n",
    "            \n",
    "            chunks.append({\n",
    "                'text': chunk,\n",
    "                'metadata': {'start_char': start, 'end_char': end}\n",
    "            })\n",
    "            \n",
    "            start += self.chunk_size - self.chunk_overlap\n",
    "            \n",
    "        print(f\"\\nCreated {len(chunks)} chunks\")\n",
    "        print(f\"Sample chunk (first 50 chars): {chunks[0]['text'][:50]}...\")\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601055f5-ba45-48d1-b371-3adc2b134845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5193586b-c6be-446e-a586-20e28c416976",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingGenerator:\n",
    "    \"\"\"Handles text embedding generation.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.api_url = \"https://api.openai.com/v1/embeddings\"\n",
    "        \n",
    "    def create_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"Generate embedding for text.\"\"\"\n",
    "        if not text.strip():\n",
    "            raise ValueError(\"Empty text provided\")\n",
    "            \n",
    "        response = requests.post(\n",
    "            self.api_url,\n",
    "            headers={\n",
    "                \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            },\n",
    "            json={\n",
    "                \"model\": \"text-embedding-3-small\",\n",
    "                \"input\": text\n",
    "            }\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        embedding = response.json()['data'][0]['embedding']\n",
    "        print(f\"Generated embedding, sample of first 3 dimensions: {embedding[:3]}\")\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66dff8ac-9492-498e-be39-d8876c252141",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages vector storage operations.\"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str, index_name: str):\n",
    "        self.api_key = api_key\n",
    "        self.index_name = index_name\n",
    "        self.headers = {\n",
    "            \"Api-Key\": self.api_key,\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"X-Pinecone-API-Version\": \"2024-07\"\n",
    "        }\n",
    "        self.index_host_cache = {}\n",
    "        \n",
    "    def describe_index(self) -> str:\n",
    "        \"\"\"Get or retrieve cached index host.\"\"\"\n",
    "        if self.index_name in self.index_host_cache:\n",
    "            return self.index_host_cache[self.index_name]\n",
    "            \n",
    "        url = f\"https://api.pinecone.io/indexes/{self.index_name}\"\n",
    "        response = requests.get(url, headers=self.headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        host = response.json()[\"host\"]\n",
    "        self.index_host_cache[self.index_name] = host\n",
    "        print(f\"\\nConnected to Pinecone host: {host}\")\n",
    "        return host\n",
    "        \n",
    "    def query_vectors(self, \n",
    "                      query_vector: List[float], \n",
    "                      top_k: int = 3, \n",
    "                      namespace: str = \"\") -> List[Dict]:\n",
    "        \"\"\"Query for most similar vectors.\"\"\"\n",
    "        host = self.describe_index()\n",
    "        url = f\"https://{host}/query\"\n",
    "        data = {\n",
    "            \"vector\": query_vector,\n",
    "            \"topK\": top_k,\n",
    "            \"namespace\": namespace,\n",
    "            \"includeMetadata\": True\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nQuerying Pinecone for top {top_k} matches\")\n",
    "        response = requests.post(url, headers=self.headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        matches = response.json()['matches']\n",
    "        return matches\n",
    "        \n",
    "    def store_vectors(self, vectors: List[Dict], namespace: str = \"\") -> Dict:\n",
    "        \"\"\"Store vectors in Pinecone.\"\"\"\n",
    "        host = self.describe_index()\n",
    "        url = f\"https://{host}/vectors/upsert\"\n",
    "        data = {\n",
    "            \"vectors\": vectors,\n",
    "            \"namespace\": namespace\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nUploading {len(vectors)} vectors to Pinecone\")\n",
    "        print(f\"Sample vector metadata: {vectors[0]['metadata']}\")\n",
    "        \n",
    "        response = requests.post(url, headers=self.headers, json=data)    \n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        print(f\"Pinecone response: {result}\")\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5470878-e8d8-4477-9155-14737be2d8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_store_document(text: str, document_id: str, openai_key: str, pinecone_key: str, index_name: str) -> bool:\n",
    "    \"\"\"Process document and store in vector database.\"\"\"\n",
    "    try:\n",
    "        # Initialize components\n",
    "        processor = DocumentProcessor()\n",
    "        embedding_gen = EmbeddingGenerator(openai_key)\n",
    "        vector_store = VectorStore(pinecone_key, index_name)\n",
    "        \n",
    "        # Create chunks\n",
    "        chunks = processor.create_chunks(text)\n",
    "        \n",
    "        # Generate embeddings and prepare vectors\n",
    "        vectors = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            embedding = embedding_gen.create_embedding(chunk['text'])\n",
    "            if embedding:\n",
    "                vectors.append({\n",
    "                    'id': f\"{document_id}-{i}\",\n",
    "                    'values': embedding,\n",
    "                    'metadata': {\n",
    "                        'text': chunk['text'],\n",
    "                        'document_id': document_id,\n",
    "                        **chunk['metadata']\n",
    "                    }\n",
    "                })\n",
    "        \n",
    "        # Store vectors\n",
    "        if vectors:\n",
    "            vector_store.store_vectors(vectors)\n",
    "            return True\n",
    "        return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing document: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b50d7fc-9b60-4489-9447-5e5000aaeb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Part 3b: LLM with Memory and RAG ===\n",
      "Adding Retrieval Augmented Generation capability\n"
     ]
    }
   ],
   "source": [
    "# Part 3b: LLM with Memory and RAG\n",
    "print(\"\\n=== Part 3b: LLM with Memory and RAG ===\")\n",
    "print(\"Adding Retrieval Augmented Generation capability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e90e1700-b9d5-4dc3-8198-130666925c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RagLLM(MemoryLLM):\n",
    "    \"\"\"LLM with conversation memory and RAG capability.\"\"\"\n",
    "    \n",
    "    def __init__(self, openai_key: str, pinecone_key: str, index_name: str, model: str = \"gpt-4o-mini\"):\n",
    "        super().__init__(openai_key, model)\n",
    "        self.pinecone_key = pinecone_key\n",
    "        self.index_name = index_name\n",
    "        self.embedding_generator = EmbeddingGenerator(openai_key)\n",
    "        self.vector_store = VectorStore(pinecone_key, index_name)\n",
    "        \n",
    "    def _get_relevant_chunks(self, text: str, top_k: int = 3) -> List[str]:\n",
    "        \"\"\"Get relevant text chunks using vector search.\"\"\"\n",
    "        try:\n",
    "            # Get query embedding\n",
    "            query_vector = self.embedding_generator.create_embedding(text)\n",
    "            \n",
    "            # Query vector store\n",
    "            matches = self.vector_store.query_vectors(\n",
    "                query_vector=query_vector,\n",
    "                top_k=top_k\n",
    "            )\n",
    "            \n",
    "            # Extract texts from metadata\n",
    "            return [match['metadata'].get('text', '') for match in matches]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting relevant chunks: {e}\")\n",
    "            return []\n",
    "            \n",
    "    def process_message(self, message: str) -> str:\n",
    "        \"\"\"Process message with conversation history and relevant context.\"\"\"\n",
    "        try:\n",
    "            # Get relevant context\n",
    "            context_chunks = self._get_relevant_chunks(message)\n",
    "            context = \"\\n\".join(context_chunks)\n",
    "            print(f\"\\nFound {len(context_chunks)} relevant chunks\")\n",
    "            if context_chunks:\n",
    "                print(f\"Sample context (first 50 chars): {context_chunks[0][:50]}...\")\n",
    "            \n",
    "            # Prepare messages\n",
    "            messages = [{\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"\"\"You are a helpful marketing assistant. Answer just if you sure about correct info 100%. If not say you not sure. \n",
    "                Use this context when relevant:\n",
    "                {context}\n",
    "                \"\"\"\n",
    "            }]\n",
    "            messages.extend([msg.dict() for msg in self.history])\n",
    "            messages.append({\"role\": \"user\", \"content\": message})\n",
    "            \n",
    "            response = requests.post(\n",
    "                \"https://api.openai.com/v1/chat/completions\",\n",
    "                headers={\n",
    "                    \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "                    \"Content-Type\": \"application/json\"\n",
    "                },\n",
    "                json={\n",
    "                    \"model\": self.model,\n",
    "                    \"messages\": messages\n",
    "                }\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Get response and update history\n",
    "            assistant_message = response.json()['choices'][0]['message']['content']\n",
    "            self.history.append(Message(role=\"user\", content=message))\n",
    "            self.history.append(Message(role=\"assistant\", content=assistant_message))\n",
    "            \n",
    "            return assistant_message\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1621347b-5cab-4b7e-8c0b-694ecf3d3849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Document Processing ===\n",
      "\n",
      "Created 13 chunks\n",
      "Sample chunk (first 50 chars): ﻿Global Sourcing RFI Workflow\n",
      "\n",
      "\n",
      "Global Sourcing RF...\n",
      "Generated embedding, sample of first 3 dimensions: [-0.009754701, 0.023795739, 0.010035283]\n",
      "Generated embedding, sample of first 3 dimensions: [-0.04238617, 0.009184542, 0.010165098]\n",
      "Generated embedding, sample of first 3 dimensions: [-0.03158922, 0.03678677, 0.020493945]\n",
      "Generated embedding, sample of first 3 dimensions: [-0.0501792, 0.040126923, 0.057081576]\n",
      "Generated embedding, sample of first 3 dimensions: [-0.04344335, 0.011441238, 0.02943367]\n",
      "Generated embedding, sample of first 3 dimensions: [-0.021920724, 0.040596727, 0.025479855]\n",
      "Generated embedding, sample of first 3 dimensions: [-0.047885858, -0.004947266, 0.02059778]\n",
      "Generated embedding, sample of first 3 dimensions: [-0.054924604, -0.008015131, 0.035972822]\n",
      "Generated embedding, sample of first 3 dimensions: [-0.047217026, -0.02386938, 0.020438973]\n",
      "Generated embedding, sample of first 3 dimensions: [-0.039087564, 0.0032896467, 0.037615087]\n",
      "Generated embedding, sample of first 3 dimensions: [-0.021836314, -0.007994682, 0.061495576]\n",
      "Generated embedding, sample of first 3 dimensions: [0.014584729, 0.028479058, 0.05385131]\n",
      "Generated embedding, sample of first 3 dimensions: [0.012331121, 0.03933503, 0.068008]\n",
      "\n",
      "Connected to Pinecone host: augmentedllm-cxx1dj3.svc.aped-4627-b74a.pinecone.io\n",
      "\n",
      "Uploading 13 vectors to Pinecone\n",
      "Sample vector metadata: {'text': '\\ufeffGlobal Sourcing RFI Workflow\\n\\n\\nGlobal Sourcing RFI Workflow        1\\nCustomer Journey Overview        2\\n1. Complete the Global Sourcing RFI        3\\n2. Was the RFI completed?        3\\n3. Deposit Data into a CRM        3\\n4. Translate Data into Vietnamese        4\\n5. Search for suitable suppliers        4\\n6. Translate Vietnamese Data into Vietnamese        4\\n7. Do we have enough suppliers?        4\\n8. Do we have duplicates?        4\\n9. Generate a Global Sourcing RFI Report        5\\n10. Generate a Global Sourcing RFI Email        5\\n11. Do we sent the Global Sourcing RFI Email?        5\\n12. Send Global Sourcing RFI Email        5\\n\\n\\n________________\\nCustomer Journey Overview \\n\\n\\nThis document provides a comprehensive workflow outline for Vietnam Direct Sourcing, a Vietnam-based platform specializing in supplier identification and matching. Vientam Direct Sourcing is designed to connect medium-sized importers, primarily from the US, Canada, Australia, and the UK, with Vietnamese factories ca', 'document_id': 'sourcing_workflow', 'start_char': 0, 'end_char': 1000}\n",
      "Pinecone response: {'upsertedCount': 13}\n",
      "\n",
      "Document processing succeeded\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Testing Document Processing ===\")\n",
    "with open(\"your doc.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "    success = process_and_store_document(\n",
    "        text=text,\n",
    "        document_id=\"yourDoc\",\n",
    "        openai_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "        pinecone_key=os.environ[\"PINECONE_API_KEY\"],\n",
    "        index_name=\"augmentedllm\"\n",
    "    )\n",
    "    print(f\"\\nDocument processing {'succeeded' if success else 'failed'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76e563c-dd6d-49eb-a2be-7416ab08ac8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99a4fbec-928b-4d2e-9693-3b51e5104013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing LLM with Memory and RAG ===\n",
      "\n",
      "User: Hi! My name is Viacheslav.\n",
      "Generated embedding, sample of first 3 dimensions: [0.015347642, -0.020478658, -0.042047087]\n",
      "\n",
      "Connected to Pinecone host: augmentedllm-cxx1dj3.svc.aped-4627-b74a.pinecone.io\n",
      "\n",
      "Querying Pinecone for top 3 matches\n",
      "\n",
      "Found 3 relevant chunks\n",
      "Sample context (first 50 chars): izing in supplier identification and matching. Vie...\n",
      "Assistant: Hello, Viacheslav! How can I assist you today?\n",
      "\n",
      "User: Do you remember my name?\n",
      "Generated embedding, sample of first 3 dimensions: [0.036457118, -0.07861758, -0.049314216]\n",
      "\n",
      "Querying Pinecone for top 3 matches\n",
      "\n",
      "Found 3 relevant chunks\n",
      "Sample context (first 50 chars): e workflow will check for available contact inform...\n",
      "Assistant: Yes, your name is Viacheslav. How can I assist you further?\n",
      "\n",
      "User: What you khow about The Global Sourcing RFI Company?\n",
      "Generated embedding, sample of first 3 dimensions: [0.0016378722, -0.012900318, 0.034605835]\n",
      "\n",
      "Querying Pinecone for top 3 matches\n",
      "\n",
      "Found 3 relevant chunks\n",
      "Sample context (first 50 chars): Sourcing RFI Email\n",
      "The Global Sourcing RFI Report ...\n",
      "Assistant: I'm not sure about specific details on \"The Global Sourcing RFI Company\" beyond the context provided regarding the Global Sourcing RFI process and the workflow for supplier identification and matching related to Vietnam Direct Sourcing. If you have specific aspects you'd like to know, please provide more details!\n",
      "\n",
      "User: We spent $10,000 on a marketing campaign that generated $50,000 in revenue with a 10% margin. What was our ROMI?\n",
      "Generated embedding, sample of first 3 dimensions: [-0.013287988, 0.07420073, 0.007861744]\n",
      "\n",
      "Querying Pinecone for top 3 matches\n",
      "\n",
      "Found 3 relevant chunks\n",
      "Sample context (first 50 chars): Sourcing RFI Email\n",
      "The Global Sourcing RFI Report ...\n",
      "Assistant: To calculate the Return on Marketing Investment (ROMI), you can use the following formula:\n",
      "\n",
      "\\[ \\text{ROMI} = \\frac{\\text{Revenue} - \\text{Marketing Investment}}{\\text{Marketing Investment}} \\]\n",
      "\n",
      "In this case:\n",
      "\n",
      "- Revenue = $50,000\n",
      "- Marketing Investment = $10,000\n",
      "\n",
      "First, calculate the profit generated:\n",
      "\n",
      "\\[ \\text{Profit} = \\text{Revenue} \\times \\text{Margin} = 50,000 \\times 0.10 = 5,000 \\]\n",
      "\n",
      "Now, plug in the values into the ROMI formula:\n",
      "\n",
      "\\[ \\text{ROMI} = \\frac{50,000 - 10,000}{10,000} = \\frac{40,000}{10,000} = 4 \\]\n",
      "\n",
      "So, the ROMI is 4, or 400%. This means for every dollar spent on marketing, you generated 4 dollars in revenue.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'To calculate the Return on Marketing Investment (ROMI), you can use the following formula:\\n\\n\\\\[ \\\\text{ROMI} = \\\\frac{\\\\text{Revenue} - \\\\text{Marketing Investment}}{\\\\text{Marketing Investment}} \\\\]\\n\\nIn this case:\\n\\n- Revenue = $50,000\\n- Marketing Investment = $10,000\\n\\nFirst, calculate the profit generated:\\n\\n\\\\[ \\\\text{Profit} = \\\\text{Revenue} \\\\times \\\\text{Margin} = 50,000 \\\\times 0.10 = 5,000 \\\\]\\n\\nNow, plug in the values into the ROMI formula:\\n\\n\\\\[ \\\\text{ROMI} = \\\\frac{50,000 - 10,000}{10,000} = \\\\frac{40,000}{10,000} = 4 \\\\]\\n\\nSo, the ROMI is 4, or 400%. This means for every dollar spent on marketing, you generated 4 dollars in revenue.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test 3b: LLM with Memory and RAG\n",
    "print(\"\\n=== Testing LLM with Memory and RAG ===\")\n",
    "rag_llm = RagLLM(os.environ[\"OPENAI_API_KEY\"], os.environ[\"PINECONE_API_KEY\"], \"augmentedllm\")\n",
    "rag_test = DialogueTest(rag_llm)\n",
    "rag_test.chat(\"Hi! My name is Viacheslav.\")\n",
    "rag_test.chat(\"Do you remember my name?\")\n",
    "rag_test.chat(\"What you khow about The Global Sourcing RFI Company?\")\n",
    "rag_test.chat(\"We spent $10,000 on a marketing campaign that generated $50,000 in revenue with a 10% margin. What was our ROMI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738c8a81-03c7-4505-ade6-22342700676c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54b155c0-edaa-4173-95c6-86054d672fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Part 4: LLM with Tools ===\n",
      "Adding marketing analytics tools\n"
     ]
    }
   ],
   "source": [
    "# Part 4: LLM with Tools\n",
    "print(\"\\n=== Part 4: LLM with Tools ===\")\n",
    "print(\"Adding marketing analytics tools\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e55867b-3458-4d59-964d-26255c9b13c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketingTools:\n",
    "    \"\"\"Provides marketing analytics calculations.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_tool_definitions() -> List[Dict]:\n",
    "        return [{\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"calculate_romi\",\n",
    "                \"description\": \"Calculate Return on Marketing Investment (ROMI)\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"revenue\": {\n",
    "                            \"type\": \"number\",\n",
    "                            \"description\": \"Total revenue generated from marketing campaign\"\n",
    "                        },\n",
    "                        \"marketing_cost\": {\n",
    "                            \"type\": \"number\",\n",
    "                            \"description\": \"Total cost of marketing campaign\"\n",
    "                        },\n",
    "                        \"margin_percent\": {\n",
    "                            \"type\": \"number\",\n",
    "                            \"description\": \"Profit margin percentage (0-100)\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"revenue\", \"marketing_cost\", \"margin_percent\"]\n",
    "                }\n",
    "            }\n",
    "        }]\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_romi(revenue: float, marketing_cost: float, margin_percent: float) -> float:\n",
    "        \"\"\"\n",
    "        Calculate ROMI using the formula: ((Revenue * Margin%) - Marketing Cost) / Marketing Cost * 100\n",
    "        Returns percentage value\n",
    "        \"\"\"\n",
    "        if marketing_cost <= 0:\n",
    "            raise ValueError(\"Marketing cost must be greater than zero\")\n",
    "        if not (0 <= margin_percent <= 100):\n",
    "            raise ValueError(\"Margin percentage must be between 0 and 100\")\n",
    "            \n",
    "        margin_multiplier = margin_percent / 100\n",
    "        profit = revenue * margin_multiplier - marketing_cost\n",
    "        romi = (profit / marketing_cost) * 100\n",
    "        return round(romi, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be382ae4-8e33-4d29-9696-af291a397a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolEnabledLLM(RagLLM):\n",
    "    \"\"\"LLM with memory, RAG and tool usage capability.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 openai_key: str, \n",
    "                 pinecone_key: str, \n",
    "                 index_name: str, \n",
    "                 model: str = \"gpt-4o-mini\"):\n",
    "        super().__init__(openai_key, pinecone_key, index_name, model)\n",
    "        self.tools = MarketingTools()\n",
    "    \n",
    "    def process_message(self, message: str) -> str:\n",
    "        \"\"\"Process message with tools, memory and RAG.\"\"\"\n",
    "        try:\n",
    "            # Get relevant context\n",
    "            context_chunks = self._get_relevant_chunks(message)\n",
    "            context = \"\\n\".join(context_chunks)\n",
    "            print(f\"\\nFound {len(context_chunks)} relevant chunks\")\n",
    "            if context_chunks:\n",
    "                print(f\"Sample context (first 100 chars): {context_chunks[0][:100]}...\")\n",
    "            \n",
    "            # Prepare messages\n",
    "            messages = [{\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"\"\"You are a marketing analytics assistant. \n",
    "                Use this context when relevant:\n",
    "                {context}\n",
    "                \n",
    "                When asked about marketing metrics, use the calculate_romi tool.\n",
    "                ROMI (Return on Marketing Investment) shows the profitability of marketing spending.\n",
    "                \"\"\"\n",
    "            }]\n",
    "            messages.extend([msg.dict() for msg in self.history])\n",
    "            messages.append({\"role\": \"user\", \"content\": message})\n",
    "            \n",
    "            # Make API request with tools\n",
    "            response = requests.post(\n",
    "                \"https://api.openai.com/v1/chat/completions\",\n",
    "                headers={\n",
    "                    \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "                    \"Content-Type\": \"application/json\"\n",
    "                },\n",
    "                json={\n",
    "                    \"model\": self.model,\n",
    "                    \"messages\": messages,\n",
    "                    \"tools\": self.tools.get_tool_definitions()\n",
    "                }\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Process response and tool calls\n",
    "            response_data = response.json()\n",
    "            assistant_message = response_data['choices'][0]['message']\n",
    "            \n",
    "            if tool_calls := assistant_message.get('tool_calls'):\n",
    "                print(\"\\nProcessing tool calls...\")\n",
    "                tool_results = self._handle_tool_calls(tool_calls)\n",
    "                \n",
    "                # Add tool results to conversation\n",
    "                messages.extend([\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": assistant_message.get('content'),\n",
    "                        \"tool_calls\": tool_calls\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"tool\",\n",
    "                        \"content\": json.dumps(tool_results),\n",
    "                        \"tool_call_id\": tool_calls[0]['id']\n",
    "                    }\n",
    "                ])\n",
    "                \n",
    "                # Get final response with tool results\n",
    "                final_response = requests.post(\n",
    "                    \"https://api.openai.com/v1/chat/completions\",\n",
    "                    headers={\n",
    "                        \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "                        \"Content-Type\": \"application/json\"\n",
    "                    },\n",
    "                    json={\n",
    "                        \"model\": self.model,\n",
    "                        \"messages\": messages\n",
    "                    }\n",
    "                )\n",
    "                final_response.raise_for_status()\n",
    "                assistant_message = final_response.json()['choices'][0]['message']\n",
    "            \n",
    "            # Update conversation history\n",
    "            self.history.append(Message(role=\"user\", content=message))\n",
    "            self.history.append(Message(role=\"assistant\", content=assistant_message['content']))\n",
    "            \n",
    "            return assistant_message['content']\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "            \n",
    "    def _handle_tool_calls(self, tool_calls: List[Dict]) -> Dict:\n",
    "        \"\"\"Process tool calls and return results.\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for call in tool_calls:\n",
    "            if call['function']['name'] == 'calculate_romi':\n",
    "                try:\n",
    "                    args = json.loads(call['function']['arguments'])\n",
    "                    result = self.tools.calculate_romi(\n",
    "                        revenue=args['revenue'],\n",
    "                        marketing_cost=args['marketing_cost'],\n",
    "                        margin_percent=args['margin_percent']\n",
    "                    )\n",
    "                    print(f\"Calculated ROMI: {result}%\")\n",
    "                    results[call['id']] = result\n",
    "                except Exception as e:\n",
    "                    results[call['id']] = f\"Error calculating ROMI: {str(e)}\"\n",
    "                    \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b1de42b0-5397-4a07-ab20-cf345ebb14c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Tool-enabled LLM ===\n",
      "\n",
      "User: Hi! My name is Slava?\n",
      "Generated embedding, sample of first 3 dimensions: [0.008466213, -0.008110435, -0.04214]\n",
      "\n",
      "Connected to Pinecone host: augmentedllm-cxx1dj3.svc.aped-4627-b74a.pinecone.io\n",
      "\n",
      "Querying Pinecone for top 3 matches\n",
      "\n",
      "Found 3 relevant chunks\n",
      "Sample context (first 100 chars): izing in supplier identification and matching. Vientam Direct Sourcing is designed to connect medium...\n",
      "Assistant: Hello Slava! How can I assist you today?\n",
      "\n",
      "User: Do you remember my name?\n",
      "Generated embedding, sample of first 3 dimensions: [0.036457118, -0.07861758, -0.049314216]\n",
      "\n",
      "Querying Pinecone for top 3 matches\n",
      "\n",
      "Found 3 relevant chunks\n",
      "Sample context (first 100 chars): e workflow will check for available contact information: if present, an email will be sent to the cl...\n",
      "Assistant: Yes, I remember your name, Slava! How can I help you today?\n",
      "\n",
      "User: How will we translate Vietnamese Data into Vietnamese?\n",
      "Generated embedding, sample of first 3 dimensions: [-0.00089565356, -0.0037716394, 0.005316328]\n",
      "\n",
      "Querying Pinecone for top 3 matches\n",
      "\n",
      "Found 3 relevant chunks\n",
      "Sample context (first 100 chars): CRM, where it creates new supplier datasets. Additionally, the AI Agent matches this newly gathered ...\n",
      "Assistant: To translate Vietnamese data into Vietnamese, the process typically involves the following steps:\n",
      "\n",
      "1. **Data Export**: The Vietnamese data gathered from various websites is exported from the CRM system.\n",
      "\n",
      "2. **AI Translation**: The exported data is sent to an AI Agent that processes and translates the data accurately into Vietnamese. Since the data is already in Vietnamese, this step may involve refining or formatting the data rather than direct translation.\n",
      "\n",
      "3. **Import Back to CRM**: After the processing and translation, the data is imported back into the CRM. This ensures that all supplier information is accessible and understandable within the system.\n",
      "\n",
      "This process helps maintain data integrity and ensures that the information is up-to-date for further processing and decision-making. If you have any specific questions or need further details, let me know!\n",
      "\n",
      "User: We spent $10,000 on a marketing campaign that generated $50,000 in revenue with a 10% margin. What was our ROMI?\n",
      "Generated embedding, sample of first 3 dimensions: [-0.013274199, 0.07419419, 0.007879979]\n",
      "\n",
      "Querying Pinecone for top 3 matches\n",
      "\n",
      "Found 3 relevant chunks\n",
      "Sample context (first 100 chars): Sourcing RFI Email\n",
      "The Global Sourcing RFI Report has been customized for a prospect buyer, tailored...\n",
      "\n",
      "Processing tool calls...\n",
      "Calculated ROMI: -50.0%\n",
      "Assistant: Your Return on Marketing Investment (ROMI) for the campaign is -50.0%. This means that the campaign did not generate a positive return relative to the costs incurred. Would you like to analyze this campaign further or need help with anything else?\n",
      "\n",
      "User: How about a campaign where we spent $200,000 and got $4,000,000 in revenue with 50% margin?\n",
      "Generated embedding, sample of first 3 dimensions: [-0.020755777, -0.011905339, 0.026280906]\n",
      "\n",
      "Querying Pinecone for top 3 matches\n",
      "\n",
      "Found 3 relevant chunks\n",
      "Sample context (first 100 chars): izing in supplier identification and matching. Vientam Direct Sourcing is designed to connect medium...\n",
      "\n",
      "Processing tool calls...\n",
      "Calculated ROMI: 900.0%\n",
      "Assistant: For the campaign where you spent $200,000 and generated $4,000,000 in revenue with a 50% margin, your ROMI is 900%. This indicates a highly profitable marketing investment. If you need further analysis or assistance, feel free to ask!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'For the campaign where you spent $200,000 and generated $4,000,000 in revenue with a 50% margin, your ROMI is 900%. This indicates a highly profitable marketing investment. If you need further analysis or assistance, feel free to ask!'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test 4: LLM with Tools\n",
    "print(\"\\n=== Testing Tool-enabled LLM ===\")\n",
    "tool_llm = ToolEnabledLLM(os.environ[\"OPENAI_API_KEY\"], os.environ[\"PINECONE_API_KEY\"], \"augmentedllm\")\n",
    "tool_test = DialogueTest(tool_llm)\n",
    "tool_test.chat(\"Hi! My name is Slava?\")\n",
    "tool_test.chat(\"Do you remember my name?\")\n",
    "tool_test.chat(\"How will we translate Vietnamese Data into Vietnamese?\")\n",
    "tool_test.chat(\"We spent $10,000 on a marketing campaign that generated $50,000 in revenue with a 10% margin. What was our ROMI?\")\n",
    "tool_test.chat(\"How about a campaign where we spent $200,000 and got $4,000,000 in revenue with 50% margin?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2babb4ff-8cdb-40bd-9617-87e9623f5a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9767b556-9b73-494b-8dfd-817331042aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
