{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ab74ff3-67da-48d9-8a3f-262f16e57c01",
   "metadata": {},
   "source": [
    "# Building an Augmented LLM Agent\n",
    "\n",
    "This notebook demonstrates the evolution of a Large Language Model (LLM) agent from a simple chatbot to a sophisticated system with memory, retrieval, and tool use capabilities. We'll follow the approach outlined in Anthropic's \"Building Effective Agents\" article, implementing each capability step by step.\n",
    "\n",
    "Key concepts we'll cover:\n",
    "1. Basic LLM interactions\n",
    "2. Adding conversation memory\n",
    "3. Implementing RAG (Retrieval Augmented Generation)\n",
    "4. Integrating specialized tools\n",
    "\n",
    "Each section builds upon the previous one, demonstrating how we can progressively enhance our LLM's capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb9933b1-7bc1-4cd2-a5ad-5d3ef3337c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сначала импортируем все необходимые библиотеки:\n",
    "# - os: для работы с переменными окружения, где мы храним наши ключи\n",
    "# - getpass: позволяет безопасно вводить секретные данные (пароли, ключи API)\n",
    "# - typing: добавляет подсказки о типах данных, делая код более понятным\n",
    "# - json: для работы с форматом JSON, который используется в API\n",
    "# - requests: популярная библиотека для отправки HTTP запросов\n",
    "# - pydantic: для валидации данных и создания моделей\n",
    "\n",
    "import os\n",
    "import getpass\n",
    "from typing import List, Dict, Optional\n",
    "import json\n",
    "import requests\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ded248de-861d-4773-8839-100cd3f77f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY:  ········\n",
      "PINECONE_API_KEY:  ········\n"
     ]
    }
   ],
   "source": [
    "# Создаем функцию для безопасной установки ключей API\n",
    "# Это важно, потому что мы не хотим хранить ключи в коде\n",
    "\n",
    "def _set_env(var: str):\n",
    "    # Проверяем, есть ли уже ключ в переменных окружения\n",
    "    if not os.environ.get(var):\n",
    "        # Если нет - запрашиваем его у пользователя безопасным способом\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "# Запрашиваем необходимые ключи API:\n",
    "# 1. OPENAI_API_KEY - для доступа к GPT моделям и эмбеддингам\n",
    "# 2. PINECONE_API_KEY - для доступа к векторной базе данных\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "_set_env(\"PINECONE_API_KEY\")\n",
    "\n",
    "# [VIDEO] Задаем имя индекса в Pinecone для хранения наших векторов\n",
    "# Pinecone - это векторная база данных, где мы храним эмбеддинги текста\n",
    "index_name = \"augmentedllm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "210d06aa-a8a9-4a90-a13a-4a49cfc78b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем класс для тестирования диалогов с агентом\n",
    "# Этот класс помогает более удобочитаемо отображать переписку между пользователем и агентом\n",
    "\n",
    "# Helper class for running dialogue tests\n",
    "class DialogueTest:\n",
    "    \"\"\"Simple framework for testing LLM dialogue capabilities.\"\"\"\n",
    "    \n",
    "    def __init__(self, agent):\n",
    "        \"\"\"Initialize with an LLM agent to test.\"\"\"\n",
    "        # agent - это наш бот (LLM), который будет отвечать на сообщения\n",
    "        # Мы сохраняем его в self.agent, чтобы использовать позже\n",
    "        self.agent = agent\n",
    "    \n",
    "    def chat(self, message: str) -> str:\n",
    "        \"\"\"Send a message and get the response.\"\"\"\n",
    "        # Метод chat:\n",
    "        # 1. Принимает сообщение от пользователя\n",
    "        # 2. Удобно выводит это сообщение\n",
    "        print(f\"\\nUser: {message}\")\n",
    "        # 3. Получает ответ от агента через process_message\n",
    "        response = self.agent.process_message(message)\n",
    "        # 4. Выводит ответ\n",
    "        print(f\"Assistant: {response}\")\n",
    "        # 5. Возвращает ответ для дальнейшего использования\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582ed6c0-25d3-4db6-91d6-34a0760dbb98",
   "metadata": {},
   "source": [
    "## Part 1: Basic LLM Implementation\n",
    "\n",
    "In this first step, we create a simple chatbot that can interact with OpenAI's API. This basic implementation:\n",
    "- Makes individual API calls for each message\n",
    "- Has no memory of previous interactions\n",
    "- Uses a simple system prompt\n",
    "\n",
    "Key Points:\n",
    "- Each interaction is independent\n",
    "- No context preservation between messages\n",
    "- Demonstrates bare minimum for LLM interaction\n",
    "\n",
    "Let's see how this basic version works and why we need more sophisticated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f30281d-b8ca-4995-bb5d-3494b6376f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем базовый класс для работы с LLM (OpenAI)\n",
    "# Это самая простая версия бота, которая просто отправляет запросы к OpenAI\n",
    "class BasicLLM:\n",
    "    \"\"\"Basic LLM with just chat capability.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, model: str = \"gpt-4o-mini\"):\n",
    "        # При создании бота указываем:\n",
    "        # - api_key: ключ для доступа к API OpenAI\n",
    "        # - model: какую модель использовать (по умолчанию gpt-4o-mini)\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "        \n",
    "    def process_message(self, message: str) -> str:\n",
    "        \"\"\"Process a single message.\"\"\"\n",
    "        # Главный метод для обработки сообщений пользователя:\n",
    "        try:\n",
    "            # Создаем базовый запрос к OpenAI API\n",
    "            response = requests.post(\n",
    "                # URL API OpenAI для чат-моделей\n",
    "                \"https://api.openai.com/v1/chat/completions\",\n",
    "                # Заголовки:\n",
    "                # - Authorization: указываем наш ключ API\n",
    "                # - Content-Type: говорим, что отправляем JSON\n",
    "                headers={\n",
    "                    \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "                    \"Content-Type\": \"application/json\"\n",
    "                },\n",
    "                json={\n",
    "                    \"model\": self.model, # Какую модель использовать\n",
    "                    # Формируем тело запроса:\n",
    "                    # - system content Системное сообщение - задает роль боту\n",
    "                    # - user content содержит сообщение пользователя\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": \"You are a helpful AI assistant.\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": message\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "            # Проверяем успешность запроса\n",
    "            response.raise_for_status()\n",
    "            # Извлекаем текст ответа из JSON:\n",
    "            # 1. response.json() - преобразуем ответ в словарь\n",
    "            # 2. ['choices'][0] - берем первый (и единственный) вариант ответа\n",
    "            # 3. ['message']['content'] - получаем текст ответа\n",
    "            return response.json()['choices'][0]['message']['content']\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Если что-то пошло не так:\n",
    "            # - Логируем ошибку\n",
    "            # - Возвращаем понятное сообщение\n",
    "            return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3feb2884-e179-4297-a2e2-6f3295ff85d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Basic LLM:\n",
      "\n",
      "User: Hi! My name is Alice.\n",
      "Assistant: Hi Alice! How can I assist you today?\n",
      "\n",
      "User: Do you remember my name?\n",
      "Assistant: I don’t have the ability to remember personal details or past interactions, including names. How can I assist you today?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I don’t have the ability to remember personal details or past interactions, including names. How can I assist you today?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Basic LLM\n",
    "basic_llm = BasicLLM(os.environ[\"OPENAI_API_KEY\"])\n",
    "basic_test = DialogueTest(basic_llm)\n",
    "\n",
    "print(\"\\nTesting Basic LLM:\")\n",
    "basic_test.chat(\"Hi! My name is Viacheslav.\")\n",
    "basic_test.chat(\"Do you remember my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8932b400-9c51-40ce-8feb-b020759a086c",
   "metadata": {},
   "source": [
    "## Part 2: Adding Conversation Memory\n",
    "\n",
    "Now we enhance our LLM with conversation memory. This allows the agent to:\n",
    "- Remember previous interactions\n",
    "- Maintain context throughout a conversation\n",
    "- Provide more coherent and contextual responses\n",
    "\n",
    "Key Improvements:\n",
    "- Stores conversation history\n",
    "- Includes past messages in new requests\n",
    "- Uses Pydantic for structured data management\n",
    "\n",
    "This step significantly improves the agent's ability to maintain meaningful conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e14c228-c484-4369-ab6a-56916d50b94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем класс Message с помощью Pydantic\n",
    "# Pydantic - это библиотека, которая помогает нам:\n",
    "# 1. Проверять правильность данных\n",
    "# 2. Автоматически преобразовывать данные в нужный формат\n",
    "# 3. Делать код более надежным\n",
    "class Message(BaseModel):\n",
    "    \"\"\"Single message in conversation history.\"\"\"\n",
    "    # У каждого сообщения есть:\n",
    "    # - role: кто отправил (system/user/assistant)\n",
    "    # - content: текст сообщения\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "# Создаем улучшенную версию Агента с памятью.\n",
    "# Для этого наследуемся от BasicLLM (получаем всю его функциональность)\n",
    "# и добавляем новые возможности\n",
    "class MemoryLLM(BasicLLM):\n",
    "    \"\"\"LLM with conversation memory.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, model: str = \"gpt-4o-mini\"):\n",
    "        # При создании:\n",
    "        # 1. Вызываем инициализацию родительского класса (super)\n",
    "        # 2. Создаем пустой список для хранения истории\n",
    "        super().__init__(api_key, model)\n",
    "        # history будет хранить объекты типа Message\n",
    "        # List[Message] означает \"список объектов класса Message\"\n",
    "        self.history: List[Message] = []\n",
    "        \n",
    "    def process_message(self, message: str) -> str:\n",
    "        \"\"\"Process message with conversation history.\"\"\"\n",
    "        try:\n",
    "            # Prepare messages including history\n",
    "            # Готовим список сообщений для API:\n",
    "            # 1. Системное сообщение (задаем роль ассистента)\n",
    "            messages = [{\n",
    "                \"role\": \"system\", \n",
    "                \"content\": \"You are a helpful marketing assistant. Answer just if you sure about correct info 100%. If not say you not sure.\"\n",
    "            }]\n",
    "\n",
    "            # Добавляем историю разговора:\n",
    "            # 1. Берем все сообщения из self.history\n",
    "            # 2. Преобразуем каждое в словарь через msg.dict()\n",
    "            # 3. Добавляем их в список сообщений\n",
    "            messages.extend([msg.dict() for msg in self.history])\n",
    "            # Добавляем текущее сообщение пользователя\n",
    "            messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "            # Отправляем запрос к API:\n",
    "            response = requests.post(\n",
    "                \"https://api.openai.com/v1/chat/completions\",\n",
    "                headers={\n",
    "                    \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "                    \"Content-Type\": \"application/json\"\n",
    "                },\n",
    "                json={\n",
    "                    \"model\": self.model,\n",
    "                    \"messages\": messages # В отличие от BasicLLM, теперь мы отправляем всю историю разговора\n",
    "                }\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Get response and update history\n",
    "            # Получаем ответ и обновляем историю:\n",
    "            # 1. Извлекаем текст ответа из JSON\n",
    "            assistant_message = response.json()['choices'][0]['message']['content']\n",
    "            # Сохраняем в историю:\n",
    "            # 1. Сообщение пользователя\n",
    "            self.history.append(Message(role=\"user\", content=message))\n",
    "            # 2. Ответ ассистента\n",
    "            self.history.append(Message(role=\"assistant\", content=assistant_message))\n",
    "            \n",
    "            return assistant_message\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac2be4e9-5448-4f92-a938-f0244b16325d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing LLM with Memory ===\n",
      "\n",
      "User: Hi! My name is Viacheslav.\n",
      "Assistant: Hello Viacheslav! How can I assist you today?\n",
      "\n",
      "User: Do you remember my name?\n",
      "Assistant: Yes, your name is Viacheslav. How can I help you today?\n",
      "\n",
      "User: What you khow about The Global Sourcing RFI Company?\n",
      "Assistant: I'm not sure about specific current details regarding The Global Sourcing RFI Company, as my training data only goes up to October 2023, and I don't have direct knowledge about every company. If you need information about their services, reputation, or other specific inquiries, I recommend checking their official website or recent news articles for the most accurate and up-to-date information.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm not sure about specific current details regarding The Global Sourcing RFI Company, as my training data only goes up to October 2023, and I don't have direct knowledge about every company. If you need information about their services, reputation, or other specific inquiries, I recommend checking their official website or recent news articles for the most accurate and up-to-date information.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Test 2: LLM with Memory\n",
    "print(\"\\n=== Testing LLM with Memory ===\")\n",
    "memory_llm = MemoryLLM(os.environ[\"OPENAI_API_KEY\"])\n",
    "memory_test = DialogueTest(memory_llm)\n",
    "memory_test.chat(\"Hi! My name is Viacheslav.\")\n",
    "memory_test.chat(\"Do you remember my name?\") \n",
    "memory_test.chat(\"What you khow about The Global Sourcing RFI Company?\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a445b8b-c560-484f-9144-b12dcde3049b",
   "metadata": {},
   "source": [
    "## Part 3a: Implementing RAG - Document Processing\n",
    "\n",
    "RAG (Retrieval Augmented Generation) enhances our LLM with the ability to use external knowledge. This section focuses on processing and storing documents:\n",
    "\n",
    "Key Components:\n",
    "1. Document Chunking: Breaking documents into manageable pieces\n",
    "2. Embedding Generation: Converting text to vector representations\n",
    "3. Vector Storage: Storing embeddings in Pinecone for efficient retrieval\n",
    "\n",
    "This step sets up the infrastructure for knowledge-enhanced responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5df9ac96-3dbd-439d-bc99-f9ef5b626704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DocumentProcessor - это класс для разбивки общего текста на маленькие части (чанки)\n",
    "# Это нужно потому что:\n",
    "# 1. Большие тексты не помещаются в контекст LLM целиком\n",
    "# 2. Маленькие части легче искать и сравнивать\n",
    "# 3. Можно находить только релевантные части текста\n",
    "class DocumentProcessor:\n",
    "    \"\"\"Processes documents into chunks.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 chunk_size: int = 1000, \n",
    "                 chunk_overlap: int = 200):\n",
    "        # При создании указываем:\n",
    "        # - chunk_size: сколько символов будет в одном кусочке\n",
    "        # - chunk_overlap: сколько символов будет пересекаться между кусочками\n",
    "        # Пересечение нужно, чтобы не разрывать предложения на середине\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "\n",
    "    def create_chunks(self, text: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Split text into overlapping chunks.\"\"\"\n",
    "        # Сначала проверяем входные данные:\n",
    "        # 1. Текст не должен быть пустым\n",
    "        if not text:\n",
    "            raise ValueError(\"Empty text provided\")\n",
    "        # 2. Размер чанка должен быть больше перекрытия\n",
    "        if self.chunk_size <= self.chunk_overlap:\n",
    "            raise ValueError(\"chunk_size must be greater than chunk_overlap\")\n",
    "        \n",
    "        chunks = []\n",
    "        start = 0\n",
    "        # Алгоритм разбивки:\n",
    "        # 1. Берем кусок (chunk) текста размером chunk_size\n",
    "        # 2. Добавляем его (chunks.append) в список вместе с метаданными\n",
    "        # 3. Сдвигаемся вперед на (chunk_size - chunk_overlap)\n",
    "        # Повторяем пока не обработаем весь текст\n",
    "        while start < len(text):\n",
    "            end = start + self.chunk_size\n",
    "            chunk = text[start:end]     \n",
    "            chunks.append({\n",
    "                # Каждый чанк - это словарь с:\n",
    "                # - text: сам текст\n",
    "                # - metadata: дополнительная информация (где начинается и заканчивается)\n",
    "                'text': chunk,\n",
    "                'metadata': {'start_char': start, 'end_char': end}\n",
    "            })\n",
    "            \n",
    "            start += self.chunk_size - self.chunk_overlap\n",
    "\n",
    "        # Выводим информацию для отладки\n",
    "        print(f\"\\nCreated {len(chunks)} chunks\")\n",
    "        print(f\"Sample chunk (first 50 chars): {chunks[0]['text'][:50]}...\")\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5193586b-c6be-446e-a586-20e28c416976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EmbeddingGenerator - класс для создания векторных представлений текста\n",
    "# Эмбеддинги - это способ представить текст в виде чисел (векторов)\n",
    "# Это нужно, чтобы:\n",
    "# 1. Можно было сравнивать тексты математически\n",
    "# 2. Находить похожие тексты быстро\n",
    "# 3. Хранить тексты в векторной базе данных\n",
    "class EmbeddingGenerator:\n",
    "    \"\"\"Handles text embedding generation.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str):\n",
    "        # Настраиваем доступ к API OpenAI:\n",
    "        # 1. Сохраняем ключ API\n",
    "        # 2. Задаем URL для создания эмбеддингов\n",
    "        self.api_key = api_key\n",
    "        self.api_url = \"https://api.openai.com/v1/embeddings\"\n",
    "        \n",
    "    def create_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"Generate embedding for text.\"\"\"\n",
    "        # Проверяем, что текст не пустой\n",
    "        if not text.strip():\n",
    "            raise ValueError(\"Empty text provided\")\n",
    "\n",
    "        # [Отправляем запрос к API OpenAI:\n",
    "        # 1. URL для создания эмбеддингов\n",
    "        # 2. Заголовки с ключом API\n",
    "        # 3. json Данные: модель и текст\n",
    "        response = requests.post(\n",
    "            self.api_url,\n",
    "            headers={\n",
    "                \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            },\n",
    "            json={\n",
    "                # Используем модель text-embedding-3-small\n",
    "                # Она создает вектора размером 1536 (!)\n",
    "                \"model\": \"text-embedding-3-small\",\n",
    "                \"input\": text\n",
    "            }\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Получаем вектор из ответа:\n",
    "        # 1. Преобразуем ответ в JSON\n",
    "        # 2. Берем первый (и единственный) эмбеддинг\n",
    "        embedding = response.json()['data'][0]['embedding']\n",
    "        # Выводим первые числа для отладки\n",
    "        print(f\"Generated embedding, sample of first 3 dimensions: {embedding[:3]}\")\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66dff8ac-9492-498e-be39-d8876c252141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VectorStore - класс для работы с векторной базой данных Pinecone\n",
    "# Pinecone помогает:\n",
    "# 1. Хранить эмбеддинги (вектора)\n",
    "# 2. Искать релевантные векторы\n",
    "# 3. Организовывать данные (namespace)\n",
    "class VectorStore:\n",
    "    \"\"\"Manages vector storage operations.\"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str, index_name: str):\n",
    "        # Настраиваем подключение к Pinecone:\n",
    "        # Сохраняем ключ API и имя индекса\n",
    "        self.api_key = api_key\n",
    "        self.index_name = index_name\n",
    "        # Задаем заголовки для запросов\n",
    "        self.headers = {\n",
    "            \"Api-Key\": self.api_key,\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"X-Pinecone-API-Version\": \"2024-07\"\n",
    "        }\n",
    "        # Создаем кэш для хостов (чтобы не запрашивать каждый раз)\n",
    "        self.index_host_cache = {}\n",
    "        \n",
    "    def describe_index(self) -> str:\n",
    "        \"\"\"Get or retrieve cached index host.\"\"\"\n",
    "        # Сначала проверяем кэш:\n",
    "        if self.index_name in self.index_host_cache:\n",
    "            return self.index_host_cache[self.index_name]\n",
    "\n",
    "        # Если в кэше нет, делаем запрос к API Pinecone:\n",
    "        # Формируем URL с именем индекса\n",
    "        url = f\"https://api.pinecone.io/indexes/{self.index_name}\"\n",
    "        # Отправляем GET запрос с заголовками\n",
    "        response = requests.get(url, headers=self.headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Получаем хост из ответа:\n",
    "        # Извлекаем хост из JSON\n",
    "        host = response.json()[\"host\"]\n",
    "        # Сохраняем в кэш и возвращаем хост\n",
    "        self.index_host_cache[self.index_name] = host\n",
    "        print(f\"\\nConnected to Pinecone host: {host}\")\n",
    "        return host\n",
    "        \n",
    "    def query_vectors(self, \n",
    "                      query_vector: List[float], \n",
    "                      top_k: int = 3, \n",
    "                      namespace: str = \"\") -> List[Dict]:\n",
    "        \"\"\"Query for most similar vectors.\"\"\"\n",
    "        # Сначала получаем хост индекса\n",
    "        host = self.describe_index()\n",
    "\n",
    "        # Формируем запрос на поиск:\n",
    "        url = f\"https://{host}/query\"\n",
    "        data = {\n",
    "            \"vector\": query_vector, # Вектор, который ищем \n",
    "            \"topK\": top_k, # Сколько похожих векторов вернуть\n",
    "            \"namespace\": namespace, # В какой коллекции искать\n",
    "            \"includeMetadata\": True # Включать ли метаданные в ответ\n",
    "        }\n",
    "\n",
    "        # Делаем запрос к API:\n",
    "        print(f\"\\nQuerying Pinecone for top {top_k} matches\")\n",
    "        response = requests.post(url, headers=self.headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Возвращаем найденные совпадения\n",
    "        matches = response.json()['matches']\n",
    "        return matches\n",
    "\n",
    "\n",
    "    def store_vectors(self, vectors: List[Dict], namespace: str = \"\") -> Dict:\n",
    "        \"\"\"Store vectors in Pinecone.\"\"\"\n",
    "        # Получаем хост для индекса\n",
    "        host = self.describe_index()\n",
    "        # Формируем URL для сохранения векторов\n",
    "        # 'upsert' означает \"обновить если существует, создать если нет\"\n",
    "        url = f\"https://{host}/vectors/upsert\"\n",
    "        # Готовим данные для отправки:    \n",
    "        data = {\n",
    "            \"vectors\": vectors, # - vectors: список векторов с их ID и метаданными\n",
    "            \"namespace\": namespace # - namespace: коллекция, где будем хранить (как папка)\n",
    "        }\n",
    "        \n",
    "        # Выводим информацию о загрузке:\n",
    "        print(f\"\\nUploading {len(vectors)} vectors to Pinecone\") # Сколько векторов загружаем\n",
    "        print(f\"Sample vector metadata: {vectors[0]['metadata']}\") # Пример метаданных первого вектора\n",
    "\n",
    "        # Отправляем запрос и получаем результат\n",
    "        response = requests.post(url, headers=self.headers, json=data)    \n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        print(f\"Pinecone response: {result}\")\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5470878-e8d8-4477-9155-14737be2d8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для полной обработки документа:\n",
    "# 1. Разбивает на части\n",
    "# 2. Создает эмбеддинги\n",
    "# 3. Сохраняет в базу данных\n",
    "def process_and_store_document(text: str, \n",
    "                               document_id: str, \n",
    "                               openai_key: str, \n",
    "                               pinecone_key: str, \n",
    "                               index_name: str) -> bool:\n",
    "    \"\"\"Process document and store in vector database.\"\"\"\n",
    "    try:\n",
    "        # Initialize components\n",
    "        # Создаем все необходимые компоненты:\n",
    "        # 1. DocumentProcessor - для разбивки текста\n",
    "        # 2. EmbeddingGenerator - для создания эмбеддингов\n",
    "        # 3. VectorStore - для сохранения в базу\n",
    "        processor = DocumentProcessor()\n",
    "        embedding_gen = EmbeddingGenerator(openai_key)\n",
    "        vector_store = VectorStore(pinecone_key, index_name)\n",
    "        \n",
    "        # Create chunks\n",
    "        # Разбиваем текст на части\n",
    "        chunks = processor.create_chunks(text)\n",
    "        \n",
    "        # Generate embeddings and prepare vectors\n",
    "        # Для каждой части:\n",
    "        vectors = []\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            embedding = embedding_gen.create_embedding(chunk['text']) # Создаем эмбеддинг (вектор)\n",
    "            if embedding:\n",
    "                 # Формируем структуру для сохранения и Добавляем в список vectors\n",
    "                vectors.append({\n",
    "                    'id': f\"{document_id}-{i}\", # Уникальный ID для каждой части\n",
    "                    'values': embedding, # Сам вектор\n",
    "                    'metadata': { # Дополнительная информация\n",
    "                        'text': chunk['text'], # Исходный текст\n",
    "                        'document_id': document_id, # ID документа\n",
    "                        **chunk['metadata'] # Другие метаданные\n",
    "                    }\n",
    "                })\n",
    "        \n",
    "        # Store vectors\n",
    "        # Сохраняем вектора если они есть\n",
    "        if vectors:\n",
    "            vector_store.store_vectors(vectors)\n",
    "            return True\n",
    "        return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing document: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a937c8-2682-4c34-910f-9b7b229552d6",
   "metadata": {},
   "source": [
    "## Part 3b: RAG-Enabled LLM\n",
    "\n",
    "Now we integrate the RAG capabilities with our LLM. This combination allows the agent to:\n",
    "- Search for relevant information in our document base\n",
    "- Include retrieved context in its responses\n",
    "- Provide more informed and accurate answers\n",
    "\n",
    "The key enhancement is the ability to ground responses in specific knowledge rather "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e90e1700-b9d5-4dc3-8198-130666925c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RagLLM - улучшенная версия Агента с памятью + поддержкой RAG\n",
    "# RAG позволяет:\n",
    "# 1. Искать релевантную информацию в базе знаний\n",
    "# 2. Использовать найденную информацию в ответах\n",
    "# 3. Давать более точные и информированные ответы\n",
    "class RagLLM(MemoryLLM):\n",
    "    \"\"\"LLM with conversation memory and RAG capability.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 openai_key: str, \n",
    "                 pinecone_key: str, \n",
    "                 index_name: str, \n",
    "                 model: str = \"gpt-4o-mini\"):\n",
    "        # Инициализируем:\n",
    "        # 1. Базовый класс (память диалога)\n",
    "        # 2. Компоненты для RAG (поиск информации)\n",
    "        super().__init__(openai_key, model)\n",
    "        self.pinecone_key = pinecone_key\n",
    "        self.index_name = index_name\n",
    "        self.embedding_generator = EmbeddingGenerator(openai_key)\n",
    "        self.vector_store = VectorStore(pinecone_key, index_name)\n",
    "        \n",
    "    def _get_relevant_chunks(self, text: str, top_k: int = 3) -> List[str]:\n",
    "        \"\"\"Get relevant text chunks using vector search.\"\"\"\n",
    "        try:\n",
    "            # Процесс поиска:\n",
    "            # 1. Создаем эмбеддинг для запроса\n",
    "            # Get query embedding\n",
    "            query_vector = self.embedding_generator.create_embedding(text)\n",
    "            \n",
    "            # Query vector store\n",
    "            # 2. Ищем похожие вектора в базе\n",
    "            matches = self.vector_store.query_vectors(\n",
    "                query_vector=query_vector,\n",
    "                top_k=top_k\n",
    "            )\n",
    "            \n",
    "            # Extract texts from metadata\n",
    "            # 3. Извлекаем тексты из метаданных\n",
    "            return [match['metadata'].get('text', '') for match in matches]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting relevant chunks: {e}\")\n",
    "            return []\n",
    "            \n",
    "    def process_message(self, message: str) -> str:\n",
    "        \"\"\"Process message with conversation history and relevant context.\"\"\"\n",
    "        try:\n",
    "            # Get relevant context\n",
    "            # 1. Ищем релевантную информацию\n",
    "            context_chunks = self._get_relevant_chunks(message)\n",
    "            # Объединяем найденные (3) части в один текст\n",
    "            context = \"\\n\".join(context_chunks)\n",
    "            print(f\"\\nFound {len(context_chunks)} relevant chunks\")\n",
    "            if context_chunks:\n",
    "                print(f\"Sample context (first 50 chars): {context_chunks[0][:50]}...\")\n",
    "            \n",
    "            # Prepare messages\n",
    "            # 2. Готовим сообщения для API:\n",
    "            # - Задаем Системное сообщение (промпт) с контекстом (инфа из RAG)\n",
    "            # - История диалога (память)\n",
    "            # - Текущее сообщение пользователя\n",
    "            messages = [{\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"\"\"You are a helpful marketing assistant. Answer just if you sure about correct info 100%. If not say you not sure. \n",
    "                Use this context when relevant:\n",
    "                {context}\n",
    "                \"\"\"\n",
    "            }]\n",
    "            messages.extend([msg.dict() for msg in self.history])\n",
    "            messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "            # 3. Делаем запрос к API\n",
    "            response = requests.post(\n",
    "                \"https://api.openai.com/v1/chat/completions\",\n",
    "                headers={\n",
    "                    \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "                    \"Content-Type\": \"application/json\"\n",
    "                },\n",
    "                json={\n",
    "                    \"model\": self.model,\n",
    "                    \"messages\": messages\n",
    "                }\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Get response and update history\n",
    "            # 4. Обрабатываем ответ:\n",
    "            # - Получаем текст ответа\n",
    "            assistant_message = response.json()['choices'][0]['message']['content']\n",
    "            # - Сохраняем в историю диалога\n",
    "            self.history.append(Message(role=\"user\", content=message))\n",
    "            self.history.append(Message(role=\"assistant\", content=assistant_message))\n",
    "            \n",
    "            return assistant_message\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1621347b-5cab-4b7e-8c0b-694ecf3d3849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Document Processing ===\n",
      "\n",
      "Created 13 chunks\n",
      "Sample chunk (first 50 chars): ﻿Global Sourcing RFI Workflow\n",
      "\n",
      "\n",
      "Global Sourcing RF...\n",
      "Generated embedding, sample of first 3 dimensions: [-0.009754701, 0.023795739, 0.010035283]\n",
      "Generated embedding, sample of first 3 dimensions: [-0.04238617, 0.009184542, 0.010165098]\n",
      "Generated embedding, sample of first 3 dimensions: [-0.03158922, 0.03678677, 0.020493945]\n",
      "Generated embedding, sample of first 3 dimensions: [-0.0501792, 0.040126923, 0.057081576]\n",
      "Generated embedding, sample of first 3 dimensions: [-0.04344335, 0.011441238, 0.02943367]\n",
      "Generated embedding, sample of first 3 dimensions: [-0.021920724, 0.040596727, 0.025479855]\n",
      "Generated embedding, sample of first 3 dimensions: [-0.047885858, -0.004947266, 0.02059778]\n",
      "Generated embedding, sample of first 3 dimensions: [-0.054924604, -0.008015131, 0.035972822]\n",
      "Generated embedding, sample of first 3 dimensions: [-0.047217026, -0.02386938, 0.020438973]\n",
      "Generated embedding, sample of first 3 dimensions: [-0.039087564, 0.0032896467, 0.037615087]\n",
      "Generated embedding, sample of first 3 dimensions: [-0.021836314, -0.007994682, 0.061495576]\n",
      "Generated embedding, sample of first 3 dimensions: [0.014584729, 0.028479058, 0.05385131]\n",
      "Generated embedding, sample of first 3 dimensions: [0.012331121, 0.03933503, 0.068008]\n",
      "\n",
      "Connected to Pinecone host: augmentedllm-cxx1dj3.svc.aped-4627-b74a.pinecone.io\n",
      "\n",
      "Uploading 13 vectors to Pinecone\n",
      "Sample vector metadata: {'text': '\\ufeffGlobal Sourcing RFI Workflow\\n\\n\\nGlobal Sourcing RFI Workflow        1\\nCustomer Journey Overview        2\\n1. Complete the Global Sourcing RFI        3\\n2. Was the RFI completed?        3\\n3. Deposit Data into a CRM        3\\n4. Translate Data into Vietnamese        4\\n5. Search for suitable suppliers        4\\n6. Translate Vietnamese Data into Vietnamese        4\\n7. Do we have enough suppliers?        4\\n8. Do we have duplicates?        4\\n9. Generate a Global Sourcing RFI Report        5\\n10. Generate a Global Sourcing RFI Email        5\\n11. Do we sent the Global Sourcing RFI Email?        5\\n12. Send Global Sourcing RFI Email        5\\n\\n\\n________________\\nCustomer Journey Overview \\n\\n\\nThis document provides a comprehensive workflow outline for Vietnam Direct Sourcing, a Vietnam-based platform specializing in supplier identification and matching. Vientam Direct Sourcing is designed to connect medium-sized importers, primarily from the US, Canada, Australia, and the UK, with Vietnamese factories ca', 'document_id': 'sourcing_workflow', 'start_char': 0, 'end_char': 1000}\n",
      "Pinecone response: {'upsertedCount': 13}\n",
      "\n",
      "Document processing succeeded\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Testing Document Processing ===\")\n",
    "with open(\"Global Sourcing RFI Workflow.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "    success = process_and_store_document(\n",
    "        text=text,\n",
    "        document_id=\"sourcing_workflow\",\n",
    "        openai_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "        pinecone_key=os.environ[\"PINECONE_API_KEY\"],\n",
    "        index_name=\"augmentedllm\"\n",
    "    )\n",
    "    print(f\"\\nDocument processing {'succeeded' if success else 'failed'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76e563c-dd6d-49eb-a2be-7416ab08ac8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99a4fbec-928b-4d2e-9693-3b51e5104013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing LLM with Memory and RAG ===\n",
      "\n",
      "User: Hi! My name is Viacheslav.\n",
      "Generated embedding, sample of first 3 dimensions: [0.015347642, -0.020478658, -0.042047087]\n",
      "\n",
      "Connected to Pinecone host: augmentedllm-cxx1dj3.svc.aped-4627-b74a.pinecone.io\n",
      "\n",
      "Querying Pinecone for top 3 matches\n",
      "\n",
      "Found 3 relevant chunks\n",
      "Sample context (first 50 chars): izing in supplier identification and matching. Vie...\n",
      "Assistant: Hello, Viacheslav! How can I assist you today?\n",
      "\n",
      "User: Do you remember my name?\n",
      "Generated embedding, sample of first 3 dimensions: [0.036457118, -0.07861758, -0.049314216]\n",
      "\n",
      "Querying Pinecone for top 3 matches\n",
      "\n",
      "Found 3 relevant chunks\n",
      "Sample context (first 50 chars): e workflow will check for available contact inform...\n",
      "Assistant: Yes, your name is Viacheslav. How can I assist you further?\n",
      "\n",
      "User: What you khow about The Global Sourcing RFI Company?\n",
      "Generated embedding, sample of first 3 dimensions: [0.0016378722, -0.012900318, 0.034605835]\n",
      "\n",
      "Querying Pinecone for top 3 matches\n",
      "\n",
      "Found 3 relevant chunks\n",
      "Sample context (first 50 chars): Sourcing RFI Email\n",
      "The Global Sourcing RFI Report ...\n",
      "Assistant: I'm not sure about specific details on \"The Global Sourcing RFI Company\" beyond the context provided regarding the Global Sourcing RFI process and the workflow for supplier identification and matching related to Vietnam Direct Sourcing. If you have specific aspects you'd like to know, please provide more details!\n",
      "\n",
      "User: We spent $10,000 on a marketing campaign that generated $50,000 in revenue with a 10% margin. What was our ROMI?\n",
      "Generated embedding, sample of first 3 dimensions: [-0.013287988, 0.07420073, 0.007861744]\n",
      "\n",
      "Querying Pinecone for top 3 matches\n",
      "\n",
      "Found 3 relevant chunks\n",
      "Sample context (first 50 chars): Sourcing RFI Email\n",
      "The Global Sourcing RFI Report ...\n",
      "Assistant: To calculate the Return on Marketing Investment (ROMI), you can use the following formula:\n",
      "\n",
      "\\[ \\text{ROMI} = \\frac{\\text{Revenue} - \\text{Marketing Investment}}{\\text{Marketing Investment}} \\]\n",
      "\n",
      "In this case:\n",
      "\n",
      "- Revenue = $50,000\n",
      "- Marketing Investment = $10,000\n",
      "\n",
      "First, calculate the profit generated:\n",
      "\n",
      "\\[ \\text{Profit} = \\text{Revenue} \\times \\text{Margin} = 50,000 \\times 0.10 = 5,000 \\]\n",
      "\n",
      "Now, plug in the values into the ROMI formula:\n",
      "\n",
      "\\[ \\text{ROMI} = \\frac{50,000 - 10,000}{10,000} = \\frac{40,000}{10,000} = 4 \\]\n",
      "\n",
      "So, the ROMI is 4, or 400%. This means for every dollar spent on marketing, you generated 4 dollars in revenue.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'To calculate the Return on Marketing Investment (ROMI), you can use the following formula:\\n\\n\\\\[ \\\\text{ROMI} = \\\\frac{\\\\text{Revenue} - \\\\text{Marketing Investment}}{\\\\text{Marketing Investment}} \\\\]\\n\\nIn this case:\\n\\n- Revenue = $50,000\\n- Marketing Investment = $10,000\\n\\nFirst, calculate the profit generated:\\n\\n\\\\[ \\\\text{Profit} = \\\\text{Revenue} \\\\times \\\\text{Margin} = 50,000 \\\\times 0.10 = 5,000 \\\\]\\n\\nNow, plug in the values into the ROMI formula:\\n\\n\\\\[ \\\\text{ROMI} = \\\\frac{50,000 - 10,000}{10,000} = \\\\frac{40,000}{10,000} = 4 \\\\]\\n\\nSo, the ROMI is 4, or 400%. This means for every dollar spent on marketing, you generated 4 dollars in revenue.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test 3b: LLM with Memory and RAG\n",
    "print(\"\\n=== Testing LLM with Memory and RAG ===\")\n",
    "rag_llm = RagLLM(os.environ[\"OPENAI_API_KEY\"], os.environ[\"PINECONE_API_KEY\"], \"augmentedllm\")\n",
    "rag_test = DialogueTest(rag_llm)\n",
    "rag_test.chat(\"Hi! My name is Viacheslav.\")\n",
    "rag_test.chat(\"Do you remember my name?\")\n",
    "rag_test.chat(\"What you khow about The Global Sourcing RFI Company?\")\n",
    "rag_test.chat(\"We spent $10,000 on a marketing campaign that generated $50,000 in revenue with a 10% margin. What was our ROMI?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbcb806-788a-4f50-b7a3-39e7f83c8190",
   "metadata": {},
   "source": [
    "## Part 4: Adding Specialized Tools\n",
    "\n",
    "In our final enhancement, we add specialized tools to our agent. This example implements marketing analytics tools, specifically ROMI calculation.\n",
    "\n",
    "Key Features:\n",
    "- Tool definition system\n",
    "- Automatic tool selection by the LLM\n",
    "- Result integration into responses\n",
    "\n",
    "This demonstrates how we can extend our agent with specific computational capabiliti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e55867b-3458-4d59-964d-26255c9b13c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MarketingTools - класс с инструментами для маркетинговой аналитики\n",
    "# Здесь мы определяем инструменты, которые может использовать Агент\n",
    "class MarketingTools:\n",
    "    \"\"\"Provides marketing analytics calculations.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_tool_definitions() -> List[Dict]:\n",
    "        # Определяем инструменты в формате, который понимает OpenAI API\n",
    "        # Для каждого инструмента указываем:\n",
    "        # 1. Название и описание\n",
    "        # 2. Входные параметры\n",
    "        # 3. Правила использования\n",
    "        return [{\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"calculate_romi\",\n",
    "                \"description\": \"Calculate Return on Marketing Investment (ROMI)\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"revenue\": {\n",
    "                            \"type\": \"number\",\n",
    "                            \"description\": \"Total revenue generated from marketing campaign\"\n",
    "                        },\n",
    "                        \"marketing_cost\": {\n",
    "                            \"type\": \"number\",\n",
    "                            \"description\": \"Total cost of marketing campaign\"\n",
    "                        },\n",
    "                        \"margin_percent\": {\n",
    "                            \"type\": \"number\",\n",
    "                            \"description\": \"Profit margin percentage (0-100)\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"revenue\", \"marketing_cost\", \"margin_percent\"]\n",
    "                }\n",
    "            }\n",
    "        }]\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_romi(revenue: float, marketing_cost: float, margin_percent: float) -> float:\n",
    "        \"\"\"\n",
    "        Calculate ROMI using the formula: ((Revenue * Margin%) - Marketing Cost) / Marketing Cost * 100\n",
    "        Returns percentage value\n",
    "        \"\"\"\n",
    "        # Проверяем входные данные:\n",
    "        if marketing_cost <= 0:\n",
    "            raise ValueError(\"Marketing cost must be greater than zero\")\n",
    "        if not (0 <= margin_percent <= 100):\n",
    "            raise ValueError(\"Margin percentage must be between 0 and 100\")\n",
    "\n",
    "        # Рассчитываем ROMI:\n",
    "        # 1. Переводим процент в множитель (10% -> 0.1)\n",
    "        margin_multiplier = margin_percent / 100\n",
    "        # 2. Считаем прибыль (выручка * маржа - затраты)\n",
    "        profit = revenue * margin_multiplier - marketing_cost\n",
    "        # 3. Считаем ROMI ((прибыль / затраты) * 100%)\n",
    "        romi = (profit / marketing_cost) * 100\n",
    "        # Округляем до 2 знаков после запятой\n",
    "        return round(romi, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be382ae4-8e33-4d29-9696-af291a397a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToolEnabledLLM - финальная версия нашего бота\n",
    "# Он объединяет все возможности:\n",
    "# 1. Память (от MemoryLLM)\n",
    "# 2. Поиск контекста (от RagLLM)\n",
    "# 3. Использование инструментов\n",
    "class ToolEnabledLLM(RagLLM):\n",
    "    \"\"\"LLM with memory, RAG and tool usage capability.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 openai_key: str, \n",
    "                 pinecone_key: str, \n",
    "                 index_name: str, \n",
    "                 model: str = \"gpt-4o-mini\"):\n",
    "        # При создании:\n",
    "        # 1. Инициализируем родительский класс (RagLLM)\n",
    "        super().__init__(openai_key, pinecone_key, index_name, model)\n",
    "        # 2. Добавляем инструменты\n",
    "        self.tools = MarketingTools()\n",
    "    \n",
    "    def process_message(self, message: str) -> str:\n",
    "        \"\"\"Process message with tools, memory and RAG.\"\"\"\n",
    "        try:\n",
    "            # 1. Поиск релевантного контекста:\n",
    "            # Get relevant context\n",
    "            # - Ищем релевантный контекст через RAG\n",
    "            context_chunks = self._get_relevant_chunks(message)\n",
    "            # - Выводим статистику для отладки\n",
    "            context = \"\\n\".join(context_chunks)\n",
    "            # - Выводим статистику для отладки\n",
    "            print(f\"\\nFound {len(context_chunks)} relevant chunks\")\n",
    "            if context_chunks:\n",
    "                print(f\"Sample context (first 50 chars): {context_chunks[0][:50]}...\")\n",
    "            \n",
    "            # Prepare messages\n",
    "            # 2. Готовим сообщения для API:\n",
    "            # - Создаем системный промпт с контекстом и Добавляем инструкции по использованию инструментов\n",
    "            messages = [{\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"\"\"You are a marketing analytics assistant. \n",
    "                Use this context when relevant:\n",
    "                {context}\n",
    "                \n",
    "                When asked about marketing metrics, use the calculate_romi tool.\n",
    "                ROMI (Return on Marketing Investment) shows the profitability of marketing spending.\n",
    "                \"\"\"\n",
    "            }]\n",
    "            # Добавляем историю диалога\n",
    "            messages.extend([msg.dict() for msg in self.history])\n",
    "            # Добавляем текущее сообщение\n",
    "            messages.append({\"role\": \"user\", \"content\": message})\n",
    "            \n",
    "            # Make API request with tools\n",
    "            # 3. Делаем первый запрос к API с инструментами\n",
    "            # - Отправляем сообщения и описания инструментов\n",
    "            # - LLM может решить использовать инструменты\n",
    "            response = requests.post(\n",
    "                \"https://api.openai.com/v1/chat/completions\",\n",
    "                headers={\n",
    "                    \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "                    \"Content-Type\": \"application/json\"\n",
    "                },\n",
    "                json={\n",
    "                    \"model\": self.model,\n",
    "                    \"messages\": messages,\n",
    "                    \"tools\": self.tools.get_tool_definitions() # tools (!)\n",
    "                }\n",
    "            )\n",
    "            response.raise_for_status() \n",
    "            \n",
    "            # Process response and tool calls\n",
    "            # 5. Обработка ответа:\n",
    "            # - Получаем JSON с ответом\n",
    "            # - Извлекаем сообщение ассистента\n",
    "            response_data = response.json()\n",
    "            assistant_message = response_data['choices'][0]['message']\n",
    "            # 6. Проверяем использование инструментов:\n",
    "            # Если Агент решил использовать инструмент (посчитать ROMI):\n",
    "            if tool_calls := assistant_message.get('tool_calls'):\n",
    "                # - Выводим сообщение о начале обработки\n",
    "                # - Вызываем инструменты и получаем результаты\n",
    "                print(\"\\nProcessing tool calls...\")\n",
    "                tool_results = self._handle_tool_calls(tool_calls)\n",
    "                # Add tool results to conversation\n",
    "                # 8. Добавляем результаты в контекст:\n",
    "                # - Сообщение ассистента с вызовом инструмента и Результаты работы инструмента (значение ROMI)\n",
    "                messages.extend([\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": assistant_message.get('content'),\n",
    "                        \"tool_calls\": tool_calls\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"tool\",\n",
    "                        \"content\": json.dumps(tool_results),\n",
    "                        \"tool_call_id\": tool_calls[0]['id']\n",
    "                    }\n",
    "                ])\n",
    "                \n",
    "                # Get final response with tool results\n",
    "                # 9. Второй запрос к API:\n",
    "                # - Отправляем обновленный контекст с результатами\n",
    "                # - LLM формирует финальный ответ с учетом результатов\n",
    "                final_response = requests.post(\n",
    "                    \"https://api.openai.com/v1/chat/completions\",\n",
    "                    headers={\n",
    "                        \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "                        \"Content-Type\": \"application/json\"\n",
    "                    },\n",
    "                    json={\n",
    "                        \"model\": self.model,\n",
    "                        \"messages\": messages\n",
    "                    }\n",
    "                )\n",
    "                final_response.raise_for_status()\n",
    "                assistant_message = final_response.json()['choices'][0]['message']\n",
    "            \n",
    "            # Update conversation history\n",
    "            # 10. Обновление истории:\n",
    "            # - Сохраняем сообщение пользователя\n",
    "            # - Сохраняем финальный ответ ассистента\n",
    "            self.history.append(Message(role=\"user\", content=message))\n",
    "            self.history.append(Message(role=\"assistant\", content=assistant_message['content']))\n",
    "\n",
    "            # 11. Возвращаем Финальный ответ:\n",
    "            return assistant_message['content']\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "            \n",
    "    def _handle_tool_calls(self, tool_calls: List[Dict]) -> Dict:\n",
    "        \"\"\"Process tool calls and return results.\"\"\"\n",
    "        # Обрабатываем каждый вызов инструмента:\n",
    "        # Для каждого вызова:\n",
    "        # 1. Проверяем, какой инструмент вызван\n",
    "        # 2. Получаем параметры\n",
    "        # 3. Выполняем расчет\n",
    "        results = {}\n",
    "        \n",
    "        for call in tool_calls:\n",
    "            # [VIDEO] Если вызван инструмент для расчета ROMI:\n",
    "            if call['function']['name'] == 'calculate_romi':\n",
    "                try:\n",
    "                    # Извлекаем параметры из JSON\n",
    "                    args = json.loads(call['function']['arguments'])\n",
    "                    # Вызываем функцию расчета\n",
    "                    result = self.tools.calculate_romi(\n",
    "                        revenue=args['revenue'],\n",
    "                        marketing_cost=args['marketing_cost'],\n",
    "                        margin_percent=args['margin_percent']\n",
    "                    )\n",
    "                    print(f\"Calculated ROMI: {result}%\")\n",
    "                    # Сохраняем результат\n",
    "                    results[call['id']] = result\n",
    "                except Exception as e:\n",
    "                    # Если произошла ошибка, сохраняем её\n",
    "                    results[call['id']] = f\"Error calculating ROMI: {str(e)}\"\n",
    "                    \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b1de42b0-5397-4a07-ab20-cf345ebb14c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Tool-enabled LLM ===\n",
      "\n",
      "User: Hi! My name is Slava?\n",
      "Generated embedding, sample of first 3 dimensions: [0.008466213, -0.008110435, -0.04214]\n",
      "\n",
      "Connected to Pinecone host: augmentedllm-cxx1dj3.svc.aped-4627-b74a.pinecone.io\n",
      "\n",
      "Querying Pinecone for top 3 matches\n",
      "\n",
      "Found 3 relevant chunks\n",
      "Sample context (first 100 chars): izing in supplier identification and matching. Vientam Direct Sourcing is designed to connect medium...\n",
      "Assistant: Hello Slava! How can I assist you today?\n",
      "\n",
      "User: Do you remember my name?\n",
      "Generated embedding, sample of first 3 dimensions: [0.036457118, -0.07861758, -0.049314216]\n",
      "\n",
      "Querying Pinecone for top 3 matches\n",
      "\n",
      "Found 3 relevant chunks\n",
      "Sample context (first 100 chars): e workflow will check for available contact information: if present, an email will be sent to the cl...\n",
      "Assistant: Yes, I remember your name, Slava! How can I help you today?\n",
      "\n",
      "User: How will we translate Vietnamese Data into Vietnamese?\n",
      "Generated embedding, sample of first 3 dimensions: [-0.00089565356, -0.0037716394, 0.005316328]\n",
      "\n",
      "Querying Pinecone for top 3 matches\n",
      "\n",
      "Found 3 relevant chunks\n",
      "Sample context (first 100 chars): CRM, where it creates new supplier datasets. Additionally, the AI Agent matches this newly gathered ...\n",
      "Assistant: To translate Vietnamese data into Vietnamese, the process typically involves the following steps:\n",
      "\n",
      "1. **Data Export**: The Vietnamese data gathered from various websites is exported from the CRM system.\n",
      "\n",
      "2. **AI Translation**: The exported data is sent to an AI Agent that processes and translates the data accurately into Vietnamese. Since the data is already in Vietnamese, this step may involve refining or formatting the data rather than direct translation.\n",
      "\n",
      "3. **Import Back to CRM**: After the processing and translation, the data is imported back into the CRM. This ensures that all supplier information is accessible and understandable within the system.\n",
      "\n",
      "This process helps maintain data integrity and ensures that the information is up-to-date for further processing and decision-making. If you have any specific questions or need further details, let me know!\n",
      "\n",
      "User: We spent $10,000 on a marketing campaign that generated $50,000 in revenue with a 10% margin. What was our ROMI?\n",
      "Generated embedding, sample of first 3 dimensions: [-0.013274199, 0.07419419, 0.007879979]\n",
      "\n",
      "Querying Pinecone for top 3 matches\n",
      "\n",
      "Found 3 relevant chunks\n",
      "Sample context (first 100 chars): Sourcing RFI Email\n",
      "The Global Sourcing RFI Report has been customized for a prospect buyer, tailored...\n",
      "\n",
      "Processing tool calls...\n",
      "Calculated ROMI: -50.0%\n",
      "Assistant: Your Return on Marketing Investment (ROMI) for the campaign is -50.0%. This means that the campaign did not generate a positive return relative to the costs incurred. Would you like to analyze this campaign further or need help with anything else?\n",
      "\n",
      "User: How about a campaign where we spent $200,000 and got $4,000,000 in revenue with 50% margin?\n",
      "Generated embedding, sample of first 3 dimensions: [-0.020755777, -0.011905339, 0.026280906]\n",
      "\n",
      "Querying Pinecone for top 3 matches\n",
      "\n",
      "Found 3 relevant chunks\n",
      "Sample context (first 100 chars): izing in supplier identification and matching. Vientam Direct Sourcing is designed to connect medium...\n",
      "\n",
      "Processing tool calls...\n",
      "Calculated ROMI: 900.0%\n",
      "Assistant: For the campaign where you spent $200,000 and generated $4,000,000 in revenue with a 50% margin, your ROMI is 900%. This indicates a highly profitable marketing investment. If you need further analysis or assistance, feel free to ask!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'For the campaign where you spent $200,000 and generated $4,000,000 in revenue with a 50% margin, your ROMI is 900%. This indicates a highly profitable marketing investment. If you need further analysis or assistance, feel free to ask!'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test 4: LLM with Tools\n",
    "print(\"\\n=== Testing Tool-enabled LLM ===\")\n",
    "tool_llm = ToolEnabledLLM(os.environ[\"OPENAI_API_KEY\"], os.environ[\"PINECONE_API_KEY\"], \"augmentedllm\")\n",
    "tool_test = DialogueTest(tool_llm)\n",
    "tool_test.chat(\"Hi! My name is Slava?\")\n",
    "tool_test.chat(\"Do you remember my name?\")\n",
    "tool_test.chat(\"How will we translate Vietnamese Data into Vietnamese?\")\n",
    "tool_test.chat(\"We spent $10,000 on a marketing campaign that generated $50,000 in revenue with a 10% margin. What was our ROMI?\")\n",
    "tool_test.chat(\"How about a campaign where we spent $200,000 and got $4,000,000 in revenue with 50% margin?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2babb4ff-8cdb-40bd-9617-87e9623f5a31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
